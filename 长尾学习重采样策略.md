# 不平衡学习深度神经网络训练框架

<div align="center">

**A Comprehensive Framework for Training Deep Neural Networks on Imbalanced Datasets**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.9+](https://img.shields.io/badge/PyTorch-1.9+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[English](#english) | [中文](#chinese)

</div>

---

## 📑 目录

- [项目简介](#项目简介)
- [核心特性](#核心特性)
- [系统架构](#系统架构)
- [快速开始](#快速开始)
- [详细使用指南](#详细使用指南)
- [配置文件说明](#配置文件说明)
- [模块详解](#模块详解)
- [实验流程](#实验流程)
- [结果分析](#结果分析)
- [常见问题](#常见问题)
- [性能优化](#性能优化)
- [引用](#引用)
- [许可证](#许可证)

---

## 🎯 项目简介

本项目是一个**完整的、模块化的深度学习训练框架**，专门针对**类别不平衡数据集**设计。框架整合了当前学术界和工业界最先进的不平衡学习技术，提供了从数据加载、模型训练到结果分析的完整工具链。

### 适用场景

- ✅ 长尾分布数据集（如自然图像识别）
- ✅ 医疗诊断（疾病罕见样本）
- ✅ 工业缺陷检测（正常样本远多于缺陷样本）
- ✅ 信号处理与分类（本项目示例：ADS-B信号识别）
- ✅ 欺诈检测、异常检测等场景

### 核心优势

| 特性 | 说明 |
|-----|------|
| 🔬 **学术级实现** | 集成15+篇顶会论文方法（CVPR、NeurIPS、ICCV等） |
| 🛠 **模块化设计** | 损失函数、采样策略、模型结构完全解耦，易于扩展 |
| 📊 **完整分析** | 自动生成Nature/Science期刊级别的可视化图表 |
| ⚡ **高效训练** | 支持混合精度训练、多GPU并行、渐进式策略 |
| 📝 **详尽日志** | 完整的训练日志、指标追踪、实验对比工具 |

---

## ✨ 核心特性

### 🎓 集成方法（按类别）

#### 1️⃣ 重采样策略（Re-sampling）
- **Inverse Frequency Sampling**: 逆频率采样，基础的类别平衡方法
- **Class-Balanced Sampling**: 先抽类别，再类内均匀采样
- **Square Root Sampling**: 平方根频率采样（LVIS benchmarks）
- **Power Law Sampling**: 幂律采样，alpha可调（α=0→均匀, α=1→原分布）
- **Progressive Power Sampling**: 渐进式幂律采样，训练过程中动态调整alpha

📚 **参考文献**:
- *Class-Balanced Loss Based on Effective Number of Samples* (CVPR 2019)
- *How Re-sampling Helps for Long-Tail Learning?* (NeurIPS 2023)

#### 2️⃣ 损失函数（Loss Functions）

**基础损失**:
- `CrossEntropy`: 标准交叉熵（支持label smoothing）
- `FocalLoss`: 焦点损失，降低易分样本权重

**重加权损失**:
- `ClassBalancedLoss`: 基于有效样本数的类别平衡损失
- `LDAMLoss`: 标签分布感知边界损失（LDAM-DRW）
- `ProgressiveLoss`: 渐进式重加权损失

**先验调整损失**:
- `BalancedSoftmaxLoss`: 平衡Softmax，调整后验概率
- `LogitAdjustmentLoss`: Logit调整损失，训练时加先验

**代价敏感损失**（Cost-Sensitive）:
- `CostSensitiveCE`: 代价敏感交叉熵
- `CostSensitiveExpected`: 期望代价/贝叶斯风险最小化
- `CostSensitiveFocal`: 代价敏感焦点损失

📚 **参考文献**:
- *Focal Loss for Dense Object Detection* (ICCV 2017)
- *Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss* (NeurIPS 2019)
- *Long-Tail Learning via Logit Adjustment* (ICLR 2021)

#### 3️⃣ 训练策略

**两阶段训练（Two-Stage Training）**:
- **Stage-1**: Baseline训练，学习良好的特征表示
- **Stage-2**: 
  - `CRT` (Classifier Re-Training): 冻结backbone，重训练分类器
  - `Fine-tune`: 解冻所有参数，差异化学习率微调

**三阶段校准（Optional Stage-3）**:
- **τ-normalization**: 分类器权重归一化
- **Logit Adjustment**: 推理时后验调整

📚 **参考文献**:
- *Decoupling Representation and Classifier for Long-Tailed Recognition* (ICLR 2020)
- *Classifier-Balancing in Long-Tailed Recognition* (2021)

#### 4️⃣ 模型架构

**信号处理专用**:
- `ConvNetADSB`: 深度卷积网络（8层Block，350通道）
- `ResNet1D`: 一维残差网络（2/3/4/5层可配置）
- `DilatedTCN`: 膨胀时域卷积网络

**频域专家**:
- `FrequencyDomainExpert`: 频域特征+ConvNet
- `ResNetFrequencyExpert`: 频域特征+ResNet

**混合专家（MoE）**:
- `MixtureOfExpertsConvNet`: 时域+频域+TCN专家组合
- `MixtureOfExpertsResNet`: ResNet风格的专家混合

**增强分类器头**:
- `CosineMarginClassifier`: 余弦边界分类器（LDAM-ready）
- `LogitAdjustedLinear`: 内置logit调整的线性层
- `TemperatureScaledClassifier`: 可学习温度缩放

---

## 🏗 系统架构

```
┌─────────────────────────────────────────────────────────────┐
│                       Training Pipeline                      │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  ┌────────────┐   ┌──────────┐   ┌───────────┐             │
│  │   Data     │──▶│  Model   │──▶│   Loss    │             │
│  │ (Sampling) │   │ (Encoder)│   │ (Weighted)│             │
│  └────────────┘   └──────────┘   └───────────┘             │
│        │                │               │                    │
│        │                │               │                    │
│        ▼                ▼               ▼                    │
│  ┌────────────────────────────────────────────┐             │
│  │            Training Manager                 │             │
│  │  • Optimizer  • Scheduler  • Early Stop    │             │
│  └────────────────────────────────────────────┘             │
│                         │                                    │
│                         ▼                                    │
│  ┌────────────────────────────────────────────┐             │
│  │         Stage-2 Fine-tuning                 │             │
│  │  • CRT  • Progressive Sampling              │             │
│  └────────────────────────────────────────────┘             │
│                         │                                    │
│                         ▼                                    │
│  ┌────────────────────────────────────────────┐             │
│  │      Evaluation & Visualization             │             │
│  │  • Metrics  • Confusion Matrix  • t-SNE    │             │
│  └────────────────────────────────────────────┘             │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

### 核心模块

| 模块 | 文件 | 功能 |
|------|------|------|
| 🗂 **数据加载** | `data_utils.py` | 数据集、采样器、长尾化工具 |
| 🏗 **模型定义** | `models.py` | 15+种网络架构与分类器头 |
| 💔 **损失函数** | `imbalanced_losses.py` | 10+种不平衡学习损失 |
| 🎯 **训练逻辑** | `train_eval.py` | 训练/验证/测试循环 |
| ⚙️ **优化器** | `optim_utils.py` | 优化器、调度器构建 |
| 🔄 **两阶段训练** | `stage2.py` | Stage-2策略（CRT/微调） |
| 📊 **性能分析** | `analysis.py` | 混淆矩阵、分组统计 |
| 🎨 **可视化** | `visualization.py` | 期刊级图表生成 |
| 🔧 **训练管理** | `training_utils.py` | 早停、检查点、指标追踪 |
| 📝 **日志记录** | `trainer_logging.py` | 训练日志管理 |
| 🚀 **主程序** | `main.py` | 完整训练流程 |
| 📈 **汇总分析** | `summarize.py` | 批量实验对比 |

---

## 🚀 快速开始

### 环境安装

#### 方法1: Conda（推荐）
```bash
# 创建环境
conda create -n imbalance python=3.8
conda activate imbalance

# 安装PyTorch（根据你的CUDA版本）
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# 安装其他依赖
pip install numpy pandas scikit-learn scipy
pip install matplotlib seaborn
pip install h5py hydra-core omegaconf
```

#### 方法2: pip
```bash
pip install torch torchvision
pip install numpy pandas scikit-learn scipy h5py
pip install matplotlib seaborn
pip install hydra-core omegaconf
```

#### 方法3: requirements.txt
```bash
pip install -r requirements.txt
```

`requirements.txt` 内容：
```txt
torch>=1.9.0
torchvision>=0.10.0
numpy>=1.19.0
pandas>=1.3.0
scikit-learn>=0.24.0
scipy>=1.7.0
h5py>=3.0.0
matplotlib>=3.3.0
seaborn>=0.11.0
hydra-core>=1.1.0
omegaconf>=2.1.0
```

### 数据准备

将数据文件组织如下：
```
your_project/
├── data/
│   ├── train.mat          # 训练数据（MAT/H5/NPZ格式）
│   └── test.mat           # 测试数据
├── config.yaml            # 配置文件
└── main.py                # 主程序
```

**支持的数据格式**:
- `.mat` - MATLAB文件（v7.3用h5py，其他用scipy）
- `.h5` / `.hdf5` - HDF5格式
- `.npy` / `.npz` - NumPy格式

**数据键约定**:
- 数据: `X` / `data` / `signal` / `signals`
- 标签: `Y` / `label` / `labels` / `y`

### 最小配置运行

创建 `config.yaml`:
```yaml
exp_name: "quick_start"
seed: 42
gpus: "0"

data:
  path_train: "data/train.mat"
  path_test: "data/test.mat"
  batch_size: 256

model:
  name: "ConvNetADSB"

loss:
  name: "CrossEntropy"

training:
  epochs: 100
  lr: 1e-3
```

运行：
```bash
python main.py
```

### 10分钟体验完整流程

```bash
# 1. 基线实验（Stage-1 only）
python main.py exp_name=baseline stage2.enabled=false

# 2. 改进实验（Stage-2 CRT + Cost-Sensitive）
python main.py exp_name=improved \
  stage2.enabled=true \
  stage2.mode=crt \
  stage2.loss=CostSensitiveCE \
  stage2.sampler=progressive_power

# 3. 查看对比结果
python summarize.py
```

---

## 📖 详细使用指南

### 1. 创建人工不平衡数据集

如果你的数据已经是平衡的，可以人工制造不平衡：

```yaml
create_imbalance: true    # 启用人工不平衡

data:
  imbalance_ratio: 100.0  # 不平衡比例（最多类/最少类）
```

代码会自动：
- 保留所有类别
- 按指数衰减减少样本（头部→尾部）
- 确保每个类至少有1个样本

### 2. 选择采样策略

#### 无采样（自然分布）
```yaml
sampling:
  name: "none"
```

#### 逆频率采样
```yaml
sampling:
  name: "inv_freq"  # 1/n_c
```

#### 平方根采样
```yaml
sampling:
  name: "sqrt"      # 1/√n_c
```

#### 幂律采样
```yaml
sampling:
  name: "power"
  alpha: 0.5        # α∈[0,1]: 0=均匀, 0.5=√, 1=原分布
```

#### 渐进式幂律采样（推荐）
```yaml
sampling:
  name: "progressive_power"
  alpha_start: 0.5  # 初始接近原分布
  alpha_end: 0.0    # 最终类均匀
```
训练过程中alpha线性衰减，从不平衡逐步过渡到平衡。

### 3. 选择损失函数

#### 标准交叉熵
```yaml
loss:
  name: "CrossEntropy"
  
training:
  label_smoothing: 0.1  # 可选
```

#### 焦点损失（Focal Loss）
```yaml
loss:
  name: "FocalLoss"
  focal_gamma: 2.0      # γ参数，关注难样本
  focal_alpha: 0.25     # α参数，可选的类别权重
```

#### 类别平衡损失
```yaml
loss:
  name: "ClassBalancedLoss"
  cb_beta: 0.9999       # β参数，控制有效样本数
```

#### LDAM损失
```yaml
loss:
  name: "LDAMLoss"
  ldam_max_margin: 0.5  # 最大边界
  ldam_scale: 30.0      # 缩放因子
  ldam_drw_start: 160   # DRW重加权开始epoch
```

#### Logit调整损失
```yaml
loss:
  name: "LogitAdjustmentLoss"
  logit_tau: 1.0        # τ参数，控制先验强度
```

#### 代价敏感损失（自动生成代价）
```yaml
loss:
  name: "CostSensitiveCE"
  cost_strategy: "auto"  # auto/sqrt/log/uniform/manual
```

### 4. 配置Stage-2训练

#### CRT模式（推荐）
```yaml
stage2:
  enabled: true
  mode: "crt"              # Classifier Re-Training
  epochs: 200
  lr: 1e-4
  
  # 损失函数
  loss: "CostSensitiveCE"
  cost_strategy: "auto"
  
  # 采样策略
  sampler: "progressive_power"
  alpha_start: 0.5
  alpha_end: 0.0
  
  # BN冻结
  freeze_bn: true
```

#### Fine-tune模式
```yaml
stage2:
  enabled: true
  mode: "finetune"
  epochs: 100
  lr: 1e-5              # Backbone学习率
  clf_lr_mult: 10.0     # 分类器学习率倍数
  
  loss: "CrossEntropy"
  sampler: "inv_freq"
  freeze_bn: false      # 不冻结BN
```

### 5. 模型选择

#### 深度卷积网络（默认）
```yaml
model:
  name: "ConvNetADSB"
  dropout: 0.1
  use_attention: true   # 通道注意力
  norm_kind: "auto"     # auto/bn/gn/ln
```

#### ResNet-1D
```yaml
model:
  name: "ResNet1D"
  dropout: 0.2
  use_attention: false
```

#### 膨胀TCN
```yaml
model:
  name: "DilatedTCN"
  dropout: 0.15
```

#### 混合专家（MoE）
```yaml
model:
  name: "MixtureOfExpertsConvNet"
  dropout: 0.1
  use_attention: true
```

### 6. 推理时后验调整

```yaml
evaluation:
  eval_logit_adjust: "posthoc"  # none/posthoc
  eval_logit_tau: 1.0            # τ参数
```

或使用Stage-3配置：
```yaml
stage3:
  mode: "logit_adjust"  # none/tau_norm/logit_adjust/both
  tau_norm: 1.0
  logit_tau: 1.0
```

---

## 🗂 配置文件说明

完整的配置模板：

```yaml
# ============================================================
# 基础配置
# ============================================================
exp_name: "my_experiment"
seed: 42
device: "cuda"
gpus: "0,1,2,3"
amp: true                      # 混合精度训练
console_log_interval: 1        # 日志打印间隔

create_imbalance: true         # 是否创建人工不平衡

# ============================================================
# 数据配置
# ============================================================
data:
  path_train: "data/train.mat"
  path_val: null               # null则从训练集划分
  path_test: "data/test.mat"
  val_ratio: 0.2               # 验证集比例
  
  batch_size: 1024
  num_workers: 4
  pin_memory: true
  drop_last: true
  
  target_length: 4800          # 信号长度
  normalize: true              # 归一化
  in_memory: true              # 全部加载到内存
  
  imbalance_ratio: 100.0       # 不平衡比例

# ============================================================
# 模型配置
# ============================================================
model:
  name: "ConvNetADSB"
  dropout: 0.1
  use_attention: true
  norm_kind: "auto"

# ============================================================
# Stage-1: 基线训练
# ============================================================
loss:
  name: "CrossEntropy"

sampling:
  name: "none"

training:
  epochs: 200
  lr: 1e-3
  weight_decay: 1e-4
  optimizer: "Adam"
  grad_clip: 1.0
  label_smoothing: 0.0

# ============================================================
# 学习率调度
# ============================================================
scheduler:
  name: "cosine"              # cosine/step/plateau/cosine_warmup_restarts
  warmup_epochs: 5
  warmup_multiplier: 1.0

# ============================================================
# 早停配置
# ============================================================
early_stopping:
  patience: 20
  monitor: "val_acc"          # val_loss/val_acc
  mode: "max"                 # min/max

# ============================================================
# Stage-2: 改进训练
# ============================================================
stage2:
  enabled: true
  mode: "crt"                 # crt/finetune
  epochs: 200
  
  lr: 1e-4
  weight_decay: 1e-4
  optimizer: "Adam"
  
  # 损失函数
  loss: "CostSensitiveCE"
  cost_strategy: "auto"
  
  # 采样策略
  sampler: "progressive_power"
  alpha_start: 0.5
  alpha_end: 0.0
  
  freeze_bn: true
  clf_lr_mult: 1.0

# ============================================================
# Stage-3: 校准（可选）
# ============================================================
stage3:
  mode: "none"                # none/tau_norm/logit_adjust/both
  tau_norm: 1.0
  logit_tau: 1.0

# ============================================================
# 评估配置
# ============================================================
evaluation:
  eval_logit_adjust: "none"   # none/posthoc
  eval_logit_tau: 1.0

experiment:
  grouping: "auto"            # auto/absolute/quantile
  many_thresh: 100
  few_thresh: 20

# ============================================================
# 可视化配置
# ============================================================
visualization:
  enabled: true
  dpi: 400
  panel_mm: 85                # Nature单栏=86mm
  panel_scale: 1.0
  
  max_samples: 5000           # t-SNE采样数
  
  # 基础图表
  plot_training_curves: true
  plot_learning_rate: true
  plot_class_distribution: true
  
  # 特征可视化
  plot_tsne_2d: true
  plot_tsne_3d: false
  tsne_perplexity: 30
  tsne_n_iter: 1000
  
  # 性能分析
  plot_confusion_matrix: true
  cm_normalize: true
  plot_per_class_acc: true
  plot_group_performance: true
  
  # 不平衡学习专用
  plot_sample_vs_performance: true
  plot_confidence_dist: true
  plot_top_confusions: true
  top_confusions_k: 10
  
  class_names: null           # 类别名称列表（可选）
```

---

## 🔍 模块详解

### data_utils.py - 数据加载与采样

**核心类**:
```python
# 数据集
ADSBSignalDataset(
    path='data/train.mat',
    target_length=4800,
    normalize=True,
    in_memory=True
)

# 采样器工厂
make_sampler(
    labels=train_labels,
    method='progressive_power',
    alpha_start=0.5,
    alpha_end=0.0,
    total_epochs=200
)

# 长尾化
make_long_tailed_indices(
    labels=train_labels,
    num_classes=10,
    imbalance_ratio=100.0,
    seed=42
)
```

**数据增强**:
- `RandomTimeShift`: 时间平移
- `RandomAmplitude`: 幅度缩放+偏置
- `RandomGaussianNoise`: 高斯噪声
- `RandomCropOrPad`: 裁剪/补零

### imbalanced_losses.py - 损失函数

**使用示例**:
```python
from imbalanced_losses import create_loss

# 创建损失函数
criterion = create_loss(
    'CostSensitiveCE',
    cost_strategy='auto',
    class_counts=class_counts
)

# 训练中使用
loss = criterion(logits, targets)
```

**高级用法 - 组合损失**:
```python
losses = {
    'ce': create_loss('CrossEntropy'),
    'focal': create_loss('FocalLoss', gamma=2.0)
}
weights = {'ce': 0.7, 'focal': 0.3}

criterion = create_loss('CombinedLoss', 
                       losses=losses, 
                       weights=weights)
```

### models.py - 模型架构

**创建模型**:
```python
from models import create_model

model = create_model(
    'ConvNetADSB',
    num_classes=10,
    dropout_rate=0.1,
    use_attention=True,
    norm_kind='auto'
)
```

**不平衡学习初始化**:
```python
from models import init_model_for_imbalanced

model = init_model_for_imbalanced(
    model,
    class_counts=class_counts,
    init_type='balanced_xavier',
    use_log_prior_bias=True,
    temperature=1.0
)
```

**更换分类器头**:
```python
from models import swap_classifier

# LDAM风格余弦分类器
swap_classifier(
    model,
    head='cosine_ldam',
    class_counts=class_counts,
    scale=30.0,
    ldam_power=0.25,
    ldam_max_m=0.5
)

# 或Logit Adjustment头
swap_classifier(
    model,
    head='logit_adjust',
    class_counts=class_counts,
    tau=1.2
)
```

### analysis.py - 性能分析

```python
from analysis import ClassificationAnalyzer

analyzer = ClassificationAnalyzer(
    class_counts=class_counts,
    grouping='auto',
    many_thresh=100,
    few_thresh=20
)

# 分析预测结果
results = analyzer.analyze_predictions(
    y_true=true_labels,
    y_pred=predictions,
    prob=probabilities  # softmax输出
)

# 结果包含：
# - overall: 整体指标
# - group_wise: 分组指标（多数类/中等类/少数类）
# - per_class: 每个类别的详细指标
# - confusion_matrix: 混淆矩阵
```

### visualization.py - 可视化

**自动生成**（在main.py末尾调用）:
```python
from visualization import visualize_all_results

visualize_all_results(
    model=model,
    test_loader=test_loader,
    device=device,
    save_dir='experiments/my_exp/results',
    logs_dir='experiments/my_exp/logs',
    test_results=test_results,
    class_counts=class_counts,
    config=viz_config,
    class_names=class_names
)
```

**手动重新生成**:
```bash
python visualization.py --exp_dir experiments/my_exp
```

生成的图表：
- `training_curves.png/pdf` - 训练/验证曲线
- `learning_rate.png/pdf` - 学习率变化
- `class_distribution.png/pdf` - 类别分布（按组着色）
- `tsne_2d.png/pdf` - t-SNE特征可视化
- `confusion_matrix.png/pdf` - 混淆矩阵
- `per_class_recall.png/pdf` - 每类召回率
- `group_performance.png/pdf` - 分组性能对比
- `sample_vs_recall.png/pdf` - 样本数vs召回率
- `sample_vs_f1.png/pdf` - 样本数vs F1
- `confidence_dist.png/pdf` - 置信度分布
- `top_confusions.png/pdf` - Top-K混淆对

---

## 🔬 实验流程

### 单次实验

```bash
# 基础实验
python main.py exp_name=baseline

# 修改参数
python main.py \
  exp_name=custom \
  data.batch_size=512 \
  training.epochs=150 \
  stage2.loss=FocalLoss
```

### 批量实验（Sweep）

创建 `run_sweep.sh`:
```bash
#!/bin/bash

# 定义实验参数
LOSSES=("CrossEntropy" "FocalLoss" "CostSensitiveCE")
SAMPLERS=("none" "inv_freq" "progressive_power")

# 遍历组合
for loss in "${LOSSES[@]}"; do
  for sampler in "${SAMPLERS[@]}"; do
    exp_name="sweep_${loss}_${sampler}"
    
    echo "Running: $exp_name"
    
    python main.py \
      exp_name=$exp_name \
      stage2.loss=$loss \
      stage2.sampler=$sampler \
      gpus="0" \
      > logs/${exp_name}.log 2>&1
      
    echo "Finished: $exp_name"
    echo "---"
  done
done

echo "All experiments completed!"
```

运行：
```bash
chmod +x run_sweep.sh
./run_sweep.sh
```

### 消融实验

**实验1: 仅采样**
```bash
python main.py exp_name=ablation_sample_only \
  stage2.enabled=true \
  stage2.loss=CrossEntropy \
  stage2.sampler=progressive_power
```

**实验2: 仅损失**
```bash
python main.py exp_name=ablation_loss_only \
  stage2.enabled=true \
  stage2.loss=CostSensitiveCE \
  stage2.sampler=none
```

**实验3: 组合**
```bash
python main.py exp_name=ablation_combined \
  stage2.enabled=true \
  stage2.loss=CostSensitiveCE \
  stage2.sampler=progressive_power
```

---

## 📊 结果分析

### 查看单个实验结果

```bash
# 文本摘要
cat experiments/my_exp_latest/results/summary.txt

# JSON详细结果
cat experiments/my_exp_latest/results/results.json

# 训练日志
cat experiments/my_exp_latest/logs/training.log
```

### 批量实验对比

```bash
# 运行汇总分析
python summarize.py

# 生成文件（在 analysis_results_时间戳/ 目录）：
# - REPORT.md                      # Markdown报告
# - ranking_overall.csv            # 总体排名
# - comparison_by_type.csv         # 按类型对比
# - comparison_sampling.csv        # 采样方法对比
# - comparison_loss.csv            # 损失函数对比
# - matrix_oa.csv                  # OA组合矩阵
# - matrix_macc.csv               # mAcc组合矩阵
# - matrix_hm.csv                 # HM组合矩阵
# - best_configurations.csv        # 最佳配置
# - improvement_over_baseline.csv  # 相对基线提升
# - all_results_full.csv          # 完整结果表
```

### 关键指标解释

| 指标 | 全称 | 说明 | 重要性 |
|-----|------|-----|--------|
| **OA** | Overall Accuracy | 总体准确率 | ⭐⭐⭐ 常规指标 |
| **mAcc** | Mean Per-Class Accuracy | 平均类别准确率 | ⭐⭐⭐⭐⭐ 不平衡学习关键 |
| **Macro-F1** | Macro F1-Score | 宏平均F1 | ⭐⭐⭐⭐ 综合性能 |
| **Balanced-Acc** | Balanced Accuracy | 平衡准确率 | ⭐⭐⭐⭐ sklearn标准 |
| **Many-Acc** | Majority Class Accuracy | 多数类准确率 | ⭐⭐⭐ 头部性能 |
| **Few-Acc** | Minority Class Accuracy | 少数类准确率 | ⭐⭐⭐⭐⭐ 尾部性能 |
| **HM** | Harmonic Mean | 调和平均（Many vs Few） | ⭐⭐⭐⭐⭐ 平衡指标 |
| **G-Mean** | Geometric Mean of Recalls | 召回率几何平均 | ⭐⭐⭐⭐ 理论重要 |

**推荐关注**:
- 论文报告：**mAcc, Many-Acc, Few-Acc, HM**
- 实际应用：**Macro-F1, Few-Acc**
- 综合评估：**HM** (平衡头部和尾部)

### 可视化分析工作流

1. **训练过程分析**
   - 查看 `training_curves.png`: 是否过拟合？验证曲线是否收敛？
   - 查看 `learning_rate.png`: 学习率衰减是否合理？

2. **数据分布分析**
   - 查看 `class_distribution.png`: 不平衡程度如何？
   - 注意多数类（蓝色）、中等类（绿色）、少数类（橙色）

3. **性能瓶颈分析**
   - 查看 `per_class_recall.png`: 哪些类别性能差？
   - 查看 `sample_vs_recall.png`: 样本数与性能的相关性
   - 查看 `top_confusions.png`: 最易混淆的类别对

4. **模型行为分析**
   - 查看 `tsne_2d.png`: 特征空间是否有良好的类别分离？
   - 查看 `confidence_dist.png`: 模型是否对错误预测过于自信？
   - 查看 `confusion_matrix.png`: 系统性错误模式

---

## ❓ 常见问题

### Q1: CUDA out of memory

**症状**: 训练时显存不足

**解决方案**:
```yaml
# 方案1: 减小批大小
data:
  batch_size: 256  # 原来1024

# 方案2: 减少workers
data:
  num_workers: 2   # 原来4

# 方案3: 禁用混合精度（如果GPU不支持）
amp: false

# 方案4: 使用更小的模型
model:
  name: "ResNet1D"
  dropout: 0.1
```

### Q2: 训练速度慢

**原因分析**:
1. 数据加载瓶颈
2. GPU利用率低
3. 采样策略开销大

**解决方案**:
```yaml
# 加速数据加载
data:
  in_memory: true      # 全部加载到内存
  num_workers: 8       # 增加worker数
  pin_memory: true     # 启用pin_memory

# 启用混合精度
amp: true

# 减少验证频率（修改代码）
# 在main.py中，添加 if epoch % 5 == 0: 来控制验证
```

### Q3: 验证集准确率不提升

**可能原因**:
1. 学习率过大/过小
2. 过拟合
3. 数据增强不足

**解决方案**:
```yaml
# 调整学习率
training:
  lr: 5e-4          # 尝试更小的学习率

# 增加正则化
training:
  weight_decay: 5e-4  # 增加权重衰减
  
model:
  dropout: 0.3      # 增加dropout

# 早停防止过拟合
early_stopping:
  patience: 15      # 减小patience
```

### Q4: Stage-2效果不明显

**可能原因**:
1. Stage-1训练不充分
2. Stage-2策略选择不当
3. 超参数不合适

**解决方案**:
```yaml
# 确保Stage-1充分训练
training:
  epochs: 300       # 增加epoch

# CRT模式配置
stage2:
  mode: "crt"
  epochs: 200       # Stage-2也要足够长
  lr: 1e-4          # 学习率要比Stage-1小
  
  # 组合策略
  loss: "CostSensitiveCE"
  sampler: "progressive_power"
```

### Q5: 少数类性能仍然很差

**解决方案**:
```yaml
# 更激进的采样
sampling:
  name: "class_uniform"  # 类均匀采样

# 更高的代价权重
loss:
  name: "CostSensitiveCE"
  cost_strategy: "auto"  # 自动逆频率

# Stage-2也使用强平衡策略
stage2:
  sampler: "class_uniform"
  loss: "CostSensitiveFocal"
  focal_gamma: 3.0         # 增大gamma
```

### Q6: 推理时性能下降

**原因**: 训练和测试分布不一致

**解决方案**:
```yaml
# 启用推理时后验调整
evaluation:
  eval_logit_adjust: "posthoc"
  eval_logit_tau: 1.0

# 或使用Stage-3
stage3:
  mode: "logit_adjust"
  logit_tau: 1.0
```

### Q7: 无法读取数据文件

**错误信息**: `FileNotFoundError` 或 `KeyError`

**解决方案**:
```python
# 检查文件路径
import os
print(os.path.exists('data/train.mat'))  # 应该返回True

# 检查MAT文件内容
import scipy.io
data = scipy.io.loadmat('data/train.mat')
print(data.keys())  # 查看可用的键

# 或用h5py（v7.3格式）
import h5py
with h5py.File('data/train.mat', 'r') as f:
    print(list(f.keys()))
```

然后在配置中指定正确的键：
```yaml
# 如果你的数据键名不是X/Y
# 需要在代码中修改ADSBSignalDataset的data_key和label_key参数
```

### Q8: 多GPU训练报错

**错误**: `RuntimeError: Expected all tensors to be on the same device`

**解决方案**:
```yaml
# 确保正确指定GPU
gpus: "0,1,2,3"  # 使用的GPU ID

# 检查代码中的DataParallel
# 在main.py中已正确处理：
# if torch.cuda.is_available() and len(gpu_ids) > 1:
#     model = nn.DataParallel(model, device_ids=gpu_ids)
```

---

## ⚡ 性能优化

### 训练加速技巧

#### 1. 混合精度训练
```yaml
amp: true  # 启用自动混合精度

# 预期加速：1.5-2x（在支持Tensor Core的GPU上）
```

#### 2. 数据加载优化
```yaml
data:
  in_memory: true      # 小数据集全加载到内存
  num_workers: 8       # CPU核心数 - 2
  pin_memory: true     # CUDA加速
  drop_last: true      # 跳过最后不完整的batch
```

#### 3. 模型优化
```python
# 使用更高效的归一化
model:
  norm_kind: "gn"  # GroupNorm比BatchNorm在小batch下更稳定
```

#### 4. 梯度累积（模拟大batch）
```python
# 在train_eval.py中添加（如果需要）
accumulation_steps = 4
for i, (x, y) in enumerate(loader):
    loss = criterion(model(x), y) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 内存优化

#### 1. 减少显存占用
```yaml
# 小batch + 梯度累积
data:
  batch_size: 256

# 不加载全部数据到内存
data:
  in_memory: false

# t-SNE只采样部分数据
visualization:
  max_samples: 1000
```

#### 2. 模型检查点策略
```python
# 只保存最佳模型
checkpointer = ModelCheckpointer(
    save_dir=ckpt_dir,
    save_best=True,
    save_interval=0,      # 不保存中间epoch
    max_checkpoints=1     # 只保留1个
)
```

### 实验效率提升

#### 1. 快速原型验证
```yaml
# 小规模快速实验
training:
  epochs: 50           # 减少epoch
  
visualization:
  enabled: false       # 先不生成图表
  
stage2:
  enabled: false       # 先只测Stage-1
```

#### 2. 并行实验（多卡）
```bash
# 不同GPU运行不同实验
CUDA_VISIBLE_DEVICES=0 python main.py exp_name=exp1 &
CUDA_VISIBLE_DEVICES=1 python main.py exp_name=exp2 &
CUDA_VISIBLE_DEVICES=2 python main.py exp_name=exp3 &
wait
```

#### 3. 使用tmux/screen持久化训练
```bash
# 创建会话
tmux new -s training

# 运行训练
python main.py exp_name=long_run

# 分离会话（训练继续）
Ctrl+B, D

# 重新连接
tmux attach -t training
```

---

## 📚 引用

如果本项目对您的研究有帮助，欢迎引用相关论文：

### 核心方法论文

**重采样方法**:
```bibtex
@inproceedings{cui2019classbalanced,
  title={Class-Balanced Loss Based on Effective Number of Samples},
  author={Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  booktitle={CVPR},
  year={2019}
}
```

**损失函数**:
```bibtex
@inproceedings{cao2019ldam,
  title={Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss},
  author={Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{menon2021logit,
  title={Long-tail Learning via Logit Adjustment},
  author={Menon, Aditya Krishna and Jayasumana, Sadeep and Rawat, Ankit Singh and Jain, Himanshu and Veit, Andreas and Kumar, Sanjiv},
  booktitle={ICLR},
  year={2021}
}
```

**两阶段训练**:
```bibtex
@inproceedings{kang2020decoupling,
  title={Decoupling Representation and Classifier for Long-Tailed Recognition},
  author={Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
  booktitle={ICLR},
  year={2020}
}
```

---

## 📄 许可证

本项目采用 **MIT License** 开源协议。

```
MIT License

Copyright (c) 2025 [Your Name/Organization]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## 🤝 贡献指南

欢迎贡献代码、报告Bug或提出新功能建议！

### 贡献方式

1. **Fork** 本仓库
2. 创建你的特性分支 (`git checkout -b feature/AmazingFeature`)
3. 提交你的修改 (`git commit -m 'Add some AmazingFeature'`)
4. 推送到分支 (`git push origin feature/AmazingFeature`)
5. 打开一个 **Pull Request**

### 代码规范

- 遵循PEP 8代码风格
- 添加必要的注释和文档字符串
- 包含单元测试（如适用）
- 更新README（如有新功能）

---

## 📮 联系方式

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Email**: your.email@example.com
- **论文**: [arXiv链接]

---

## 🙏 致谢

本项目在开发过程中参考了以下优秀开源项目：

- [PyTorch](https://pytorch.org/)
- [scikit-learn](https://scikit-learn.org/)
- [Hydra](https://hydra.cc/)
- 各种CVPR/NeurIPS论文的官方实现

特别感谢所有贡献者和使用者的反馈！

---

## 📌 更新日志

### v1.0.0 (2025-01-08)
- ✨ 初始版本发布
- 🎯 集成15+种不平衡学习方法
- 📊 Nature/Science期刊级可视化
- 🚀 两阶段训练框架
- 📝 完整文档和示例

### TODO
- [ ] 添加更多模型架构（Transformer、GNN等）
- [ ] 支持多标签分类
- [ ] 集成自动超参数搜索（Optuna）
- [ ] 添加知识蒸馏支持
- [ ] 在线学习/增量学习模式
- [ ] Docker容器化部署

---

<div align="center">

**⭐ 如果觉得有用，请给个Star支持一下！⭐**

Made with ❤️ by [Your Name]

</div>