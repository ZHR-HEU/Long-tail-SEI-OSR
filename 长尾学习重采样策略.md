# ä¸å¹³è¡¡å­¦ä¹ æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒæ¡†æ¶

<div align="center">

**A Comprehensive Framework for Training Deep Neural Networks on Imbalanced Datasets**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.9+](https://img.shields.io/badge/PyTorch-1.9+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[English](#english) | [ä¸­æ–‡](#chinese)

</div>

---

## ğŸ“‘ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†ä½¿ç”¨æŒ‡å—](#è¯¦ç»†ä½¿ç”¨æŒ‡å—)
- [é…ç½®æ–‡ä»¶è¯´æ˜](#é…ç½®æ–‡ä»¶è¯´æ˜)
- [æ¨¡å—è¯¦è§£](#æ¨¡å—è¯¦è§£)
- [å®éªŒæµç¨‹](#å®éªŒæµç¨‹)
- [ç»“æœåˆ†æ](#ç»“æœåˆ†æ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¼•ç”¨](#å¼•ç”¨)
- [è®¸å¯è¯](#è®¸å¯è¯)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ª**å®Œæ•´çš„ã€æ¨¡å—åŒ–çš„æ·±åº¦å­¦ä¹ è®­ç»ƒæ¡†æ¶**ï¼Œä¸“é—¨é’ˆå¯¹**ç±»åˆ«ä¸å¹³è¡¡æ•°æ®é›†**è®¾è®¡ã€‚æ¡†æ¶æ•´åˆäº†å½“å‰å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæœ€å…ˆè¿›çš„ä¸å¹³è¡¡å­¦ä¹ æŠ€æœ¯ï¼Œæä¾›äº†ä»æ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒåˆ°ç»“æœåˆ†æçš„å®Œæ•´å·¥å…·é“¾ã€‚

### é€‚ç”¨åœºæ™¯

- âœ… é•¿å°¾åˆ†å¸ƒæ•°æ®é›†ï¼ˆå¦‚è‡ªç„¶å›¾åƒè¯†åˆ«ï¼‰
- âœ… åŒ»ç–—è¯Šæ–­ï¼ˆç–¾ç—…ç½•è§æ ·æœ¬ï¼‰
- âœ… å·¥ä¸šç¼ºé™·æ£€æµ‹ï¼ˆæ­£å¸¸æ ·æœ¬è¿œå¤šäºç¼ºé™·æ ·æœ¬ï¼‰
- âœ… ä¿¡å·å¤„ç†ä¸åˆ†ç±»ï¼ˆæœ¬é¡¹ç›®ç¤ºä¾‹ï¼šADS-Bä¿¡å·è¯†åˆ«ï¼‰
- âœ… æ¬ºè¯ˆæ£€æµ‹ã€å¼‚å¸¸æ£€æµ‹ç­‰åœºæ™¯

### æ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§ | è¯´æ˜ |
|-----|------|
| ğŸ”¬ **å­¦æœ¯çº§å®ç°** | é›†æˆ15+ç¯‡é¡¶ä¼šè®ºæ–‡æ–¹æ³•ï¼ˆCVPRã€NeurIPSã€ICCVç­‰ï¼‰ |
| ğŸ›  **æ¨¡å—åŒ–è®¾è®¡** | æŸå¤±å‡½æ•°ã€é‡‡æ ·ç­–ç•¥ã€æ¨¡å‹ç»“æ„å®Œå…¨è§£è€¦ï¼Œæ˜“äºæ‰©å±• |
| ğŸ“Š **å®Œæ•´åˆ†æ** | è‡ªåŠ¨ç”ŸæˆNature/ScienceæœŸåˆŠçº§åˆ«çš„å¯è§†åŒ–å›¾è¡¨ |
| âš¡ **é«˜æ•ˆè®­ç»ƒ** | æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€å¤šGPUå¹¶è¡Œã€æ¸è¿›å¼ç­–ç•¥ |
| ğŸ“ **è¯¦å°½æ—¥å¿—** | å®Œæ•´çš„è®­ç»ƒæ—¥å¿—ã€æŒ‡æ ‡è¿½è¸ªã€å®éªŒå¯¹æ¯”å·¥å…· |

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ“ é›†æˆæ–¹æ³•ï¼ˆæŒ‰ç±»åˆ«ï¼‰

#### 1ï¸âƒ£ é‡é‡‡æ ·ç­–ç•¥ï¼ˆRe-samplingï¼‰
- **Inverse Frequency Sampling**: é€†é¢‘ç‡é‡‡æ ·ï¼ŒåŸºç¡€çš„ç±»åˆ«å¹³è¡¡æ–¹æ³•
- **Class-Balanced Sampling**: å…ˆæŠ½ç±»åˆ«ï¼Œå†ç±»å†…å‡åŒ€é‡‡æ ·
- **Square Root Sampling**: å¹³æ–¹æ ¹é¢‘ç‡é‡‡æ ·ï¼ˆLVIS benchmarksï¼‰
- **Power Law Sampling**: å¹‚å¾‹é‡‡æ ·ï¼Œalphaå¯è°ƒï¼ˆÎ±=0â†’å‡åŒ€, Î±=1â†’åŸåˆ†å¸ƒï¼‰
- **Progressive Power Sampling**: æ¸è¿›å¼å¹‚å¾‹é‡‡æ ·ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´alpha

ğŸ“š **å‚è€ƒæ–‡çŒ®**:
- *Class-Balanced Loss Based on Effective Number of Samples* (CVPR 2019)
- *How Re-sampling Helps for Long-Tail Learning?* (NeurIPS 2023)

#### 2ï¸âƒ£ æŸå¤±å‡½æ•°ï¼ˆLoss Functionsï¼‰

**åŸºç¡€æŸå¤±**:
- `CrossEntropy`: æ ‡å‡†äº¤å‰ç†µï¼ˆæ”¯æŒlabel smoothingï¼‰
- `FocalLoss`: ç„¦ç‚¹æŸå¤±ï¼Œé™ä½æ˜“åˆ†æ ·æœ¬æƒé‡

**é‡åŠ æƒæŸå¤±**:
- `ClassBalancedLoss`: åŸºäºæœ‰æ•ˆæ ·æœ¬æ•°çš„ç±»åˆ«å¹³è¡¡æŸå¤±
- `LDAMLoss`: æ ‡ç­¾åˆ†å¸ƒæ„ŸçŸ¥è¾¹ç•ŒæŸå¤±ï¼ˆLDAM-DRWï¼‰
- `ProgressiveLoss`: æ¸è¿›å¼é‡åŠ æƒæŸå¤±

**å…ˆéªŒè°ƒæ•´æŸå¤±**:
- `BalancedSoftmaxLoss`: å¹³è¡¡Softmaxï¼Œè°ƒæ•´åéªŒæ¦‚ç‡
- `LogitAdjustmentLoss`: Logitè°ƒæ•´æŸå¤±ï¼Œè®­ç»ƒæ—¶åŠ å…ˆéªŒ

**ä»£ä»·æ•æ„ŸæŸå¤±**ï¼ˆCost-Sensitiveï¼‰:
- `CostSensitiveCE`: ä»£ä»·æ•æ„Ÿäº¤å‰ç†µ
- `CostSensitiveExpected`: æœŸæœ›ä»£ä»·/è´å¶æ–¯é£é™©æœ€å°åŒ–
- `CostSensitiveFocal`: ä»£ä»·æ•æ„Ÿç„¦ç‚¹æŸå¤±

ğŸ“š **å‚è€ƒæ–‡çŒ®**:
- *Focal Loss for Dense Object Detection* (ICCV 2017)
- *Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss* (NeurIPS 2019)
- *Long-Tail Learning via Logit Adjustment* (ICLR 2021)

#### 3ï¸âƒ£ è®­ç»ƒç­–ç•¥

**ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆTwo-Stage Trainingï¼‰**:
- **Stage-1**: Baselineè®­ç»ƒï¼Œå­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤º
- **Stage-2**: 
  - `CRT` (Classifier Re-Training): å†»ç»“backboneï¼Œé‡è®­ç»ƒåˆ†ç±»å™¨
  - `Fine-tune`: è§£å†»æ‰€æœ‰å‚æ•°ï¼Œå·®å¼‚åŒ–å­¦ä¹ ç‡å¾®è°ƒ

**ä¸‰é˜¶æ®µæ ¡å‡†ï¼ˆOptional Stage-3ï¼‰**:
- **Ï„-normalization**: åˆ†ç±»å™¨æƒé‡å½’ä¸€åŒ–
- **Logit Adjustment**: æ¨ç†æ—¶åéªŒè°ƒæ•´

ğŸ“š **å‚è€ƒæ–‡çŒ®**:
- *Decoupling Representation and Classifier for Long-Tailed Recognition* (ICLR 2020)
- *Classifier-Balancing in Long-Tailed Recognition* (2021)

#### 4ï¸âƒ£ æ¨¡å‹æ¶æ„

**ä¿¡å·å¤„ç†ä¸“ç”¨**:
- `ConvNetADSB`: æ·±åº¦å·ç§¯ç½‘ç»œï¼ˆ8å±‚Blockï¼Œ350é€šé“ï¼‰
- `ResNet1D`: ä¸€ç»´æ®‹å·®ç½‘ç»œï¼ˆ2/3/4/5å±‚å¯é…ç½®ï¼‰
- `DilatedTCN`: è†¨èƒ€æ—¶åŸŸå·ç§¯ç½‘ç»œ

**é¢‘åŸŸä¸“å®¶**:
- `FrequencyDomainExpert`: é¢‘åŸŸç‰¹å¾+ConvNet
- `ResNetFrequencyExpert`: é¢‘åŸŸç‰¹å¾+ResNet

**æ··åˆä¸“å®¶ï¼ˆMoEï¼‰**:
- `MixtureOfExpertsConvNet`: æ—¶åŸŸ+é¢‘åŸŸ+TCNä¸“å®¶ç»„åˆ
- `MixtureOfExpertsResNet`: ResNeté£æ ¼çš„ä¸“å®¶æ··åˆ

**å¢å¼ºåˆ†ç±»å™¨å¤´**:
- `CosineMarginClassifier`: ä½™å¼¦è¾¹ç•Œåˆ†ç±»å™¨ï¼ˆLDAM-readyï¼‰
- `LogitAdjustedLinear`: å†…ç½®logitè°ƒæ•´çš„çº¿æ€§å±‚
- `TemperatureScaledClassifier`: å¯å­¦ä¹ æ¸©åº¦ç¼©æ”¾

---

## ğŸ— ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Training Pipeline                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Data     â”‚â”€â”€â–¶â”‚  Model   â”‚â”€â”€â–¶â”‚   Loss    â”‚             â”‚
â”‚  â”‚ (Sampling) â”‚   â”‚ (Encoder)â”‚   â”‚ (Weighted)â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚        â”‚                â”‚               â”‚                    â”‚
â”‚        â”‚                â”‚               â”‚                    â”‚
â”‚        â–¼                â–¼               â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚            Training Manager                 â”‚             â”‚
â”‚  â”‚  â€¢ Optimizer  â€¢ Scheduler  â€¢ Early Stop    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚         Stage-2 Fine-tuning                 â”‚             â”‚
â”‚  â”‚  â€¢ CRT  â€¢ Progressive Sampling              â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚      Evaluation & Visualization             â”‚             â”‚
â”‚  â”‚  â€¢ Metrics  â€¢ Confusion Matrix  â€¢ t-SNE    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | æ–‡ä»¶ | åŠŸèƒ½ |
|------|------|------|
| ğŸ—‚ **æ•°æ®åŠ è½½** | `data_utils.py` | æ•°æ®é›†ã€é‡‡æ ·å™¨ã€é•¿å°¾åŒ–å·¥å…· |
| ğŸ— **æ¨¡å‹å®šä¹‰** | `models.py` | 15+ç§ç½‘ç»œæ¶æ„ä¸åˆ†ç±»å™¨å¤´ |
| ğŸ’” **æŸå¤±å‡½æ•°** | `imbalanced_losses.py` | 10+ç§ä¸å¹³è¡¡å­¦ä¹ æŸå¤± |
| ğŸ¯ **è®­ç»ƒé€»è¾‘** | `train_eval.py` | è®­ç»ƒ/éªŒè¯/æµ‹è¯•å¾ªç¯ |
| âš™ï¸ **ä¼˜åŒ–å™¨** | `optim_utils.py` | ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨æ„å»º |
| ğŸ”„ **ä¸¤é˜¶æ®µè®­ç»ƒ** | `stage2.py` | Stage-2ç­–ç•¥ï¼ˆCRT/å¾®è°ƒï¼‰ |
| ğŸ“Š **æ€§èƒ½åˆ†æ** | `analysis.py` | æ··æ·†çŸ©é˜µã€åˆ†ç»„ç»Ÿè®¡ |
| ğŸ¨ **å¯è§†åŒ–** | `visualization.py` | æœŸåˆŠçº§å›¾è¡¨ç”Ÿæˆ |
| ğŸ”§ **è®­ç»ƒç®¡ç†** | `training_utils.py` | æ—©åœã€æ£€æŸ¥ç‚¹ã€æŒ‡æ ‡è¿½è¸ª |
| ğŸ“ **æ—¥å¿—è®°å½•** | `trainer_logging.py` | è®­ç»ƒæ—¥å¿—ç®¡ç† |
| ğŸš€ **ä¸»ç¨‹åº** | `main.py` | å®Œæ•´è®­ç»ƒæµç¨‹ |
| ğŸ“ˆ **æ±‡æ€»åˆ†æ** | `summarize.py` | æ‰¹é‡å®éªŒå¯¹æ¯” |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå®‰è£…

#### æ–¹æ³•1: Condaï¼ˆæ¨èï¼‰
```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n imbalance python=3.8
conda activate imbalance

# å®‰è£…PyTorchï¼ˆæ ¹æ®ä½ çš„CUDAç‰ˆæœ¬ï¼‰
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£…å…¶ä»–ä¾èµ–
pip install numpy pandas scikit-learn scipy
pip install matplotlib seaborn
pip install h5py hydra-core omegaconf
```

#### æ–¹æ³•2: pip
```bash
pip install torch torchvision
pip install numpy pandas scikit-learn scipy h5py
pip install matplotlib seaborn
pip install hydra-core omegaconf
```

#### æ–¹æ³•3: requirements.txt
```bash
pip install -r requirements.txt
```

`requirements.txt` å†…å®¹ï¼š
```txt
torch>=1.9.0
torchvision>=0.10.0
numpy>=1.19.0
pandas>=1.3.0
scikit-learn>=0.24.0
scipy>=1.7.0
h5py>=3.0.0
matplotlib>=3.3.0
seaborn>=0.11.0
hydra-core>=1.1.0
omegaconf>=2.1.0
```

### æ•°æ®å‡†å¤‡

å°†æ•°æ®æ–‡ä»¶ç»„ç»‡å¦‚ä¸‹ï¼š
```
your_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.mat          # è®­ç»ƒæ•°æ®ï¼ˆMAT/H5/NPZæ ¼å¼ï¼‰
â”‚   â””â”€â”€ test.mat           # æµ‹è¯•æ•°æ®
â”œâ”€â”€ config.yaml            # é…ç½®æ–‡ä»¶
â””â”€â”€ main.py                # ä¸»ç¨‹åº
```

**æ”¯æŒçš„æ•°æ®æ ¼å¼**:
- `.mat` - MATLABæ–‡ä»¶ï¼ˆv7.3ç”¨h5pyï¼Œå…¶ä»–ç”¨scipyï¼‰
- `.h5` / `.hdf5` - HDF5æ ¼å¼
- `.npy` / `.npz` - NumPyæ ¼å¼

**æ•°æ®é”®çº¦å®š**:
- æ•°æ®: `X` / `data` / `signal` / `signals`
- æ ‡ç­¾: `Y` / `label` / `labels` / `y`

### æœ€å°é…ç½®è¿è¡Œ

åˆ›å»º `config.yaml`:
```yaml
exp_name: "quick_start"
seed: 42
gpus: "0"

data:
  path_train: "data/train.mat"
  path_test: "data/test.mat"
  batch_size: 256

model:
  name: "ConvNetADSB"

loss:
  name: "CrossEntropy"

training:
  epochs: 100
  lr: 1e-3
```

è¿è¡Œï¼š
```bash
python main.py
```

### 10åˆ†é’Ÿä½“éªŒå®Œæ•´æµç¨‹

```bash
# 1. åŸºçº¿å®éªŒï¼ˆStage-1 onlyï¼‰
python main.py exp_name=baseline stage2.enabled=false

# 2. æ”¹è¿›å®éªŒï¼ˆStage-2 CRT + Cost-Sensitiveï¼‰
python main.py exp_name=improved \
  stage2.enabled=true \
  stage2.mode=crt \
  stage2.loss=CostSensitiveCE \
  stage2.sampler=progressive_power

# 3. æŸ¥çœ‹å¯¹æ¯”ç»“æœ
python summarize.py
```

---

## ğŸ“– è¯¦ç»†ä½¿ç”¨æŒ‡å—

### 1. åˆ›å»ºäººå·¥ä¸å¹³è¡¡æ•°æ®é›†

å¦‚æœä½ çš„æ•°æ®å·²ç»æ˜¯å¹³è¡¡çš„ï¼Œå¯ä»¥äººå·¥åˆ¶é€ ä¸å¹³è¡¡ï¼š

```yaml
create_imbalance: true    # å¯ç”¨äººå·¥ä¸å¹³è¡¡

data:
  imbalance_ratio: 100.0  # ä¸å¹³è¡¡æ¯”ä¾‹ï¼ˆæœ€å¤šç±»/æœ€å°‘ç±»ï¼‰
```

ä»£ç ä¼šè‡ªåŠ¨ï¼š
- ä¿ç•™æ‰€æœ‰ç±»åˆ«
- æŒ‰æŒ‡æ•°è¡°å‡å‡å°‘æ ·æœ¬ï¼ˆå¤´éƒ¨â†’å°¾éƒ¨ï¼‰
- ç¡®ä¿æ¯ä¸ªç±»è‡³å°‘æœ‰1ä¸ªæ ·æœ¬

### 2. é€‰æ‹©é‡‡æ ·ç­–ç•¥

#### æ— é‡‡æ ·ï¼ˆè‡ªç„¶åˆ†å¸ƒï¼‰
```yaml
sampling:
  name: "none"
```

#### é€†é¢‘ç‡é‡‡æ ·
```yaml
sampling:
  name: "inv_freq"  # 1/n_c
```

#### å¹³æ–¹æ ¹é‡‡æ ·
```yaml
sampling:
  name: "sqrt"      # 1/âˆšn_c
```

#### å¹‚å¾‹é‡‡æ ·
```yaml
sampling:
  name: "power"
  alpha: 0.5        # Î±âˆˆ[0,1]: 0=å‡åŒ€, 0.5=âˆš, 1=åŸåˆ†å¸ƒ
```

#### æ¸è¿›å¼å¹‚å¾‹é‡‡æ ·ï¼ˆæ¨èï¼‰
```yaml
sampling:
  name: "progressive_power"
  alpha_start: 0.5  # åˆå§‹æ¥è¿‘åŸåˆ†å¸ƒ
  alpha_end: 0.0    # æœ€ç»ˆç±»å‡åŒ€
```
è®­ç»ƒè¿‡ç¨‹ä¸­alphaçº¿æ€§è¡°å‡ï¼Œä»ä¸å¹³è¡¡é€æ­¥è¿‡æ¸¡åˆ°å¹³è¡¡ã€‚

### 3. é€‰æ‹©æŸå¤±å‡½æ•°

#### æ ‡å‡†äº¤å‰ç†µ
```yaml
loss:
  name: "CrossEntropy"
  
training:
  label_smoothing: 0.1  # å¯é€‰
```

#### ç„¦ç‚¹æŸå¤±ï¼ˆFocal Lossï¼‰
```yaml
loss:
  name: "FocalLoss"
  focal_gamma: 2.0      # Î³å‚æ•°ï¼Œå…³æ³¨éš¾æ ·æœ¬
  focal_alpha: 0.25     # Î±å‚æ•°ï¼Œå¯é€‰çš„ç±»åˆ«æƒé‡
```

#### ç±»åˆ«å¹³è¡¡æŸå¤±
```yaml
loss:
  name: "ClassBalancedLoss"
  cb_beta: 0.9999       # Î²å‚æ•°ï¼Œæ§åˆ¶æœ‰æ•ˆæ ·æœ¬æ•°
```

#### LDAMæŸå¤±
```yaml
loss:
  name: "LDAMLoss"
  ldam_max_margin: 0.5  # æœ€å¤§è¾¹ç•Œ
  ldam_scale: 30.0      # ç¼©æ”¾å› å­
  ldam_drw_start: 160   # DRWé‡åŠ æƒå¼€å§‹epoch
```

#### Logitè°ƒæ•´æŸå¤±
```yaml
loss:
  name: "LogitAdjustmentLoss"
  logit_tau: 1.0        # Ï„å‚æ•°ï¼Œæ§åˆ¶å…ˆéªŒå¼ºåº¦
```

#### ä»£ä»·æ•æ„ŸæŸå¤±ï¼ˆè‡ªåŠ¨ç”Ÿæˆä»£ä»·ï¼‰
```yaml
loss:
  name: "CostSensitiveCE"
  cost_strategy: "auto"  # auto/sqrt/log/uniform/manual
```

### 4. é…ç½®Stage-2è®­ç»ƒ

#### CRTæ¨¡å¼ï¼ˆæ¨èï¼‰
```yaml
stage2:
  enabled: true
  mode: "crt"              # Classifier Re-Training
  epochs: 200
  lr: 1e-4
  
  # æŸå¤±å‡½æ•°
  loss: "CostSensitiveCE"
  cost_strategy: "auto"
  
  # é‡‡æ ·ç­–ç•¥
  sampler: "progressive_power"
  alpha_start: 0.5
  alpha_end: 0.0
  
  # BNå†»ç»“
  freeze_bn: true
```

#### Fine-tuneæ¨¡å¼
```yaml
stage2:
  enabled: true
  mode: "finetune"
  epochs: 100
  lr: 1e-5              # Backboneå­¦ä¹ ç‡
  clf_lr_mult: 10.0     # åˆ†ç±»å™¨å­¦ä¹ ç‡å€æ•°
  
  loss: "CrossEntropy"
  sampler: "inv_freq"
  freeze_bn: false      # ä¸å†»ç»“BN
```

### 5. æ¨¡å‹é€‰æ‹©

#### æ·±åº¦å·ç§¯ç½‘ç»œï¼ˆé»˜è®¤ï¼‰
```yaml
model:
  name: "ConvNetADSB"
  dropout: 0.1
  use_attention: true   # é€šé“æ³¨æ„åŠ›
  norm_kind: "auto"     # auto/bn/gn/ln
```

#### ResNet-1D
```yaml
model:
  name: "ResNet1D"
  dropout: 0.2
  use_attention: false
```

#### è†¨èƒ€TCN
```yaml
model:
  name: "DilatedTCN"
  dropout: 0.15
```

#### æ··åˆä¸“å®¶ï¼ˆMoEï¼‰
```yaml
model:
  name: "MixtureOfExpertsConvNet"
  dropout: 0.1
  use_attention: true
```

### 6. æ¨ç†æ—¶åéªŒè°ƒæ•´

```yaml
evaluation:
  eval_logit_adjust: "posthoc"  # none/posthoc
  eval_logit_tau: 1.0            # Ï„å‚æ•°
```

æˆ–ä½¿ç”¨Stage-3é…ç½®ï¼š
```yaml
stage3:
  mode: "logit_adjust"  # none/tau_norm/logit_adjust/both
  tau_norm: 1.0
  logit_tau: 1.0
```

---

## ğŸ—‚ é…ç½®æ–‡ä»¶è¯´æ˜

å®Œæ•´çš„é…ç½®æ¨¡æ¿ï¼š

```yaml
# ============================================================
# åŸºç¡€é…ç½®
# ============================================================
exp_name: "my_experiment"
seed: 42
device: "cuda"
gpus: "0,1,2,3"
amp: true                      # æ··åˆç²¾åº¦è®­ç»ƒ
console_log_interval: 1        # æ—¥å¿—æ‰“å°é—´éš”

create_imbalance: true         # æ˜¯å¦åˆ›å»ºäººå·¥ä¸å¹³è¡¡

# ============================================================
# æ•°æ®é…ç½®
# ============================================================
data:
  path_train: "data/train.mat"
  path_val: null               # nullåˆ™ä»è®­ç»ƒé›†åˆ’åˆ†
  path_test: "data/test.mat"
  val_ratio: 0.2               # éªŒè¯é›†æ¯”ä¾‹
  
  batch_size: 1024
  num_workers: 4
  pin_memory: true
  drop_last: true
  
  target_length: 4800          # ä¿¡å·é•¿åº¦
  normalize: true              # å½’ä¸€åŒ–
  in_memory: true              # å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
  
  imbalance_ratio: 100.0       # ä¸å¹³è¡¡æ¯”ä¾‹

# ============================================================
# æ¨¡å‹é…ç½®
# ============================================================
model:
  name: "ConvNetADSB"
  dropout: 0.1
  use_attention: true
  norm_kind: "auto"

# ============================================================
# Stage-1: åŸºçº¿è®­ç»ƒ
# ============================================================
loss:
  name: "CrossEntropy"

sampling:
  name: "none"

training:
  epochs: 200
  lr: 1e-3
  weight_decay: 1e-4
  optimizer: "Adam"
  grad_clip: 1.0
  label_smoothing: 0.0

# ============================================================
# å­¦ä¹ ç‡è°ƒåº¦
# ============================================================
scheduler:
  name: "cosine"              # cosine/step/plateau/cosine_warmup_restarts
  warmup_epochs: 5
  warmup_multiplier: 1.0

# ============================================================
# æ—©åœé…ç½®
# ============================================================
early_stopping:
  patience: 20
  monitor: "val_acc"          # val_loss/val_acc
  mode: "max"                 # min/max

# ============================================================
# Stage-2: æ”¹è¿›è®­ç»ƒ
# ============================================================
stage2:
  enabled: true
  mode: "crt"                 # crt/finetune
  epochs: 200
  
  lr: 1e-4
  weight_decay: 1e-4
  optimizer: "Adam"
  
  # æŸå¤±å‡½æ•°
  loss: "CostSensitiveCE"
  cost_strategy: "auto"
  
  # é‡‡æ ·ç­–ç•¥
  sampler: "progressive_power"
  alpha_start: 0.5
  alpha_end: 0.0
  
  freeze_bn: true
  clf_lr_mult: 1.0

# ============================================================
# Stage-3: æ ¡å‡†ï¼ˆå¯é€‰ï¼‰
# ============================================================
stage3:
  mode: "none"                # none/tau_norm/logit_adjust/both
  tau_norm: 1.0
  logit_tau: 1.0

# ============================================================
# è¯„ä¼°é…ç½®
# ============================================================
evaluation:
  eval_logit_adjust: "none"   # none/posthoc
  eval_logit_tau: 1.0

experiment:
  grouping: "auto"            # auto/absolute/quantile
  many_thresh: 100
  few_thresh: 20

# ============================================================
# å¯è§†åŒ–é…ç½®
# ============================================================
visualization:
  enabled: true
  dpi: 400
  panel_mm: 85                # Natureå•æ =86mm
  panel_scale: 1.0
  
  max_samples: 5000           # t-SNEé‡‡æ ·æ•°
  
  # åŸºç¡€å›¾è¡¨
  plot_training_curves: true
  plot_learning_rate: true
  plot_class_distribution: true
  
  # ç‰¹å¾å¯è§†åŒ–
  plot_tsne_2d: true
  plot_tsne_3d: false
  tsne_perplexity: 30
  tsne_n_iter: 1000
  
  # æ€§èƒ½åˆ†æ
  plot_confusion_matrix: true
  cm_normalize: true
  plot_per_class_acc: true
  plot_group_performance: true
  
  # ä¸å¹³è¡¡å­¦ä¹ ä¸“ç”¨
  plot_sample_vs_performance: true
  plot_confidence_dist: true
  plot_top_confusions: true
  top_confusions_k: 10
  
  class_names: null           # ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
```

---

## ğŸ” æ¨¡å—è¯¦è§£

### data_utils.py - æ•°æ®åŠ è½½ä¸é‡‡æ ·

**æ ¸å¿ƒç±»**:
```python
# æ•°æ®é›†
ADSBSignalDataset(
    path='data/train.mat',
    target_length=4800,
    normalize=True,
    in_memory=True
)

# é‡‡æ ·å™¨å·¥å‚
make_sampler(
    labels=train_labels,
    method='progressive_power',
    alpha_start=0.5,
    alpha_end=0.0,
    total_epochs=200
)

# é•¿å°¾åŒ–
make_long_tailed_indices(
    labels=train_labels,
    num_classes=10,
    imbalance_ratio=100.0,
    seed=42
)
```

**æ•°æ®å¢å¼º**:
- `RandomTimeShift`: æ—¶é—´å¹³ç§»
- `RandomAmplitude`: å¹…åº¦ç¼©æ”¾+åç½®
- `RandomGaussianNoise`: é«˜æ–¯å™ªå£°
- `RandomCropOrPad`: è£å‰ª/è¡¥é›¶

### imbalanced_losses.py - æŸå¤±å‡½æ•°

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from imbalanced_losses import create_loss

# åˆ›å»ºæŸå¤±å‡½æ•°
criterion = create_loss(
    'CostSensitiveCE',
    cost_strategy='auto',
    class_counts=class_counts
)

# è®­ç»ƒä¸­ä½¿ç”¨
loss = criterion(logits, targets)
```

**é«˜çº§ç”¨æ³• - ç»„åˆæŸå¤±**:
```python
losses = {
    'ce': create_loss('CrossEntropy'),
    'focal': create_loss('FocalLoss', gamma=2.0)
}
weights = {'ce': 0.7, 'focal': 0.3}

criterion = create_loss('CombinedLoss', 
                       losses=losses, 
                       weights=weights)
```

### models.py - æ¨¡å‹æ¶æ„

**åˆ›å»ºæ¨¡å‹**:
```python
from models import create_model

model = create_model(
    'ConvNetADSB',
    num_classes=10,
    dropout_rate=0.1,
    use_attention=True,
    norm_kind='auto'
)
```

**ä¸å¹³è¡¡å­¦ä¹ åˆå§‹åŒ–**:
```python
from models import init_model_for_imbalanced

model = init_model_for_imbalanced(
    model,
    class_counts=class_counts,
    init_type='balanced_xavier',
    use_log_prior_bias=True,
    temperature=1.0
)
```

**æ›´æ¢åˆ†ç±»å™¨å¤´**:
```python
from models import swap_classifier

# LDAMé£æ ¼ä½™å¼¦åˆ†ç±»å™¨
swap_classifier(
    model,
    head='cosine_ldam',
    class_counts=class_counts,
    scale=30.0,
    ldam_power=0.25,
    ldam_max_m=0.5
)

# æˆ–Logit Adjustmentå¤´
swap_classifier(
    model,
    head='logit_adjust',
    class_counts=class_counts,
    tau=1.2
)
```

### analysis.py - æ€§èƒ½åˆ†æ

```python
from analysis import ClassificationAnalyzer

analyzer = ClassificationAnalyzer(
    class_counts=class_counts,
    grouping='auto',
    many_thresh=100,
    few_thresh=20
)

# åˆ†æé¢„æµ‹ç»“æœ
results = analyzer.analyze_predictions(
    y_true=true_labels,
    y_pred=predictions,
    prob=probabilities  # softmaxè¾“å‡º
)

# ç»“æœåŒ…å«ï¼š
# - overall: æ•´ä½“æŒ‡æ ‡
# - group_wise: åˆ†ç»„æŒ‡æ ‡ï¼ˆå¤šæ•°ç±»/ä¸­ç­‰ç±»/å°‘æ•°ç±»ï¼‰
# - per_class: æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
# - confusion_matrix: æ··æ·†çŸ©é˜µ
```

### visualization.py - å¯è§†åŒ–

**è‡ªåŠ¨ç”Ÿæˆ**ï¼ˆåœ¨main.pyæœ«å°¾è°ƒç”¨ï¼‰:
```python
from visualization import visualize_all_results

visualize_all_results(
    model=model,
    test_loader=test_loader,
    device=device,
    save_dir='experiments/my_exp/results',
    logs_dir='experiments/my_exp/logs',
    test_results=test_results,
    class_counts=class_counts,
    config=viz_config,
    class_names=class_names
)
```

**æ‰‹åŠ¨é‡æ–°ç”Ÿæˆ**:
```bash
python visualization.py --exp_dir experiments/my_exp
```

ç”Ÿæˆçš„å›¾è¡¨ï¼š
- `training_curves.png/pdf` - è®­ç»ƒ/éªŒè¯æ›²çº¿
- `learning_rate.png/pdf` - å­¦ä¹ ç‡å˜åŒ–
- `class_distribution.png/pdf` - ç±»åˆ«åˆ†å¸ƒï¼ˆæŒ‰ç»„ç€è‰²ï¼‰
- `tsne_2d.png/pdf` - t-SNEç‰¹å¾å¯è§†åŒ–
- `confusion_matrix.png/pdf` - æ··æ·†çŸ©é˜µ
- `per_class_recall.png/pdf` - æ¯ç±»å¬å›ç‡
- `group_performance.png/pdf` - åˆ†ç»„æ€§èƒ½å¯¹æ¯”
- `sample_vs_recall.png/pdf` - æ ·æœ¬æ•°vså¬å›ç‡
- `sample_vs_f1.png/pdf` - æ ·æœ¬æ•°vs F1
- `confidence_dist.png/pdf` - ç½®ä¿¡åº¦åˆ†å¸ƒ
- `top_confusions.png/pdf` - Top-Kæ··æ·†å¯¹

---

## ğŸ”¬ å®éªŒæµç¨‹

### å•æ¬¡å®éªŒ

```bash
# åŸºç¡€å®éªŒ
python main.py exp_name=baseline

# ä¿®æ”¹å‚æ•°
python main.py \
  exp_name=custom \
  data.batch_size=512 \
  training.epochs=150 \
  stage2.loss=FocalLoss
```

### æ‰¹é‡å®éªŒï¼ˆSweepï¼‰

åˆ›å»º `run_sweep.sh`:
```bash
#!/bin/bash

# å®šä¹‰å®éªŒå‚æ•°
LOSSES=("CrossEntropy" "FocalLoss" "CostSensitiveCE")
SAMPLERS=("none" "inv_freq" "progressive_power")

# éå†ç»„åˆ
for loss in "${LOSSES[@]}"; do
  for sampler in "${SAMPLERS[@]}"; do
    exp_name="sweep_${loss}_${sampler}"
    
    echo "Running: $exp_name"
    
    python main.py \
      exp_name=$exp_name \
      stage2.loss=$loss \
      stage2.sampler=$sampler \
      gpus="0" \
      > logs/${exp_name}.log 2>&1
      
    echo "Finished: $exp_name"
    echo "---"
  done
done

echo "All experiments completed!"
```

è¿è¡Œï¼š
```bash
chmod +x run_sweep.sh
./run_sweep.sh
```

### æ¶ˆèå®éªŒ

**å®éªŒ1: ä»…é‡‡æ ·**
```bash
python main.py exp_name=ablation_sample_only \
  stage2.enabled=true \
  stage2.loss=CrossEntropy \
  stage2.sampler=progressive_power
```

**å®éªŒ2: ä»…æŸå¤±**
```bash
python main.py exp_name=ablation_loss_only \
  stage2.enabled=true \
  stage2.loss=CostSensitiveCE \
  stage2.sampler=none
```

**å®éªŒ3: ç»„åˆ**
```bash
python main.py exp_name=ablation_combined \
  stage2.enabled=true \
  stage2.loss=CostSensitiveCE \
  stage2.sampler=progressive_power
```

---

## ğŸ“Š ç»“æœåˆ†æ

### æŸ¥çœ‹å•ä¸ªå®éªŒç»“æœ

```bash
# æ–‡æœ¬æ‘˜è¦
cat experiments/my_exp_latest/results/summary.txt

# JSONè¯¦ç»†ç»“æœ
cat experiments/my_exp_latest/results/results.json

# è®­ç»ƒæ—¥å¿—
cat experiments/my_exp_latest/logs/training.log
```

### æ‰¹é‡å®éªŒå¯¹æ¯”

```bash
# è¿è¡Œæ±‡æ€»åˆ†æ
python summarize.py

# ç”Ÿæˆæ–‡ä»¶ï¼ˆåœ¨ analysis_results_æ—¶é—´æˆ³/ ç›®å½•ï¼‰ï¼š
# - REPORT.md                      # MarkdownæŠ¥å‘Š
# - ranking_overall.csv            # æ€»ä½“æ’å
# - comparison_by_type.csv         # æŒ‰ç±»å‹å¯¹æ¯”
# - comparison_sampling.csv        # é‡‡æ ·æ–¹æ³•å¯¹æ¯”
# - comparison_loss.csv            # æŸå¤±å‡½æ•°å¯¹æ¯”
# - matrix_oa.csv                  # OAç»„åˆçŸ©é˜µ
# - matrix_macc.csv               # mAccç»„åˆçŸ©é˜µ
# - matrix_hm.csv                 # HMç»„åˆçŸ©é˜µ
# - best_configurations.csv        # æœ€ä½³é…ç½®
# - improvement_over_baseline.csv  # ç›¸å¯¹åŸºçº¿æå‡
# - all_results_full.csv          # å®Œæ•´ç»“æœè¡¨
```

### å…³é”®æŒ‡æ ‡è§£é‡Š

| æŒ‡æ ‡ | å…¨ç§° | è¯´æ˜ | é‡è¦æ€§ |
|-----|------|-----|--------|
| **OA** | Overall Accuracy | æ€»ä½“å‡†ç¡®ç‡ | â­â­â­ å¸¸è§„æŒ‡æ ‡ |
| **mAcc** | Mean Per-Class Accuracy | å¹³å‡ç±»åˆ«å‡†ç¡®ç‡ | â­â­â­â­â­ ä¸å¹³è¡¡å­¦ä¹ å…³é”® |
| **Macro-F1** | Macro F1-Score | å®å¹³å‡F1 | â­â­â­â­ ç»¼åˆæ€§èƒ½ |
| **Balanced-Acc** | Balanced Accuracy | å¹³è¡¡å‡†ç¡®ç‡ | â­â­â­â­ sklearnæ ‡å‡† |
| **Many-Acc** | Majority Class Accuracy | å¤šæ•°ç±»å‡†ç¡®ç‡ | â­â­â­ å¤´éƒ¨æ€§èƒ½ |
| **Few-Acc** | Minority Class Accuracy | å°‘æ•°ç±»å‡†ç¡®ç‡ | â­â­â­â­â­ å°¾éƒ¨æ€§èƒ½ |
| **HM** | Harmonic Mean | è°ƒå’Œå¹³å‡ï¼ˆMany vs Fewï¼‰ | â­â­â­â­â­ å¹³è¡¡æŒ‡æ ‡ |
| **G-Mean** | Geometric Mean of Recalls | å¬å›ç‡å‡ ä½•å¹³å‡ | â­â­â­â­ ç†è®ºé‡è¦ |

**æ¨èå…³æ³¨**:
- è®ºæ–‡æŠ¥å‘Šï¼š**mAcc, Many-Acc, Few-Acc, HM**
- å®é™…åº”ç”¨ï¼š**Macro-F1, Few-Acc**
- ç»¼åˆè¯„ä¼°ï¼š**HM** (å¹³è¡¡å¤´éƒ¨å’Œå°¾éƒ¨)

### å¯è§†åŒ–åˆ†æå·¥ä½œæµ

1. **è®­ç»ƒè¿‡ç¨‹åˆ†æ**
   - æŸ¥çœ‹ `training_curves.png`: æ˜¯å¦è¿‡æ‹Ÿåˆï¼ŸéªŒè¯æ›²çº¿æ˜¯å¦æ”¶æ•›ï¼Ÿ
   - æŸ¥çœ‹ `learning_rate.png`: å­¦ä¹ ç‡è¡°å‡æ˜¯å¦åˆç†ï¼Ÿ

2. **æ•°æ®åˆ†å¸ƒåˆ†æ**
   - æŸ¥çœ‹ `class_distribution.png`: ä¸å¹³è¡¡ç¨‹åº¦å¦‚ä½•ï¼Ÿ
   - æ³¨æ„å¤šæ•°ç±»ï¼ˆè“è‰²ï¼‰ã€ä¸­ç­‰ç±»ï¼ˆç»¿è‰²ï¼‰ã€å°‘æ•°ç±»ï¼ˆæ©™è‰²ï¼‰

3. **æ€§èƒ½ç“¶é¢ˆåˆ†æ**
   - æŸ¥çœ‹ `per_class_recall.png`: å“ªäº›ç±»åˆ«æ€§èƒ½å·®ï¼Ÿ
   - æŸ¥çœ‹ `sample_vs_recall.png`: æ ·æœ¬æ•°ä¸æ€§èƒ½çš„ç›¸å…³æ€§
   - æŸ¥çœ‹ `top_confusions.png`: æœ€æ˜“æ··æ·†çš„ç±»åˆ«å¯¹

4. **æ¨¡å‹è¡Œä¸ºåˆ†æ**
   - æŸ¥çœ‹ `tsne_2d.png`: ç‰¹å¾ç©ºé—´æ˜¯å¦æœ‰è‰¯å¥½çš„ç±»åˆ«åˆ†ç¦»ï¼Ÿ
   - æŸ¥çœ‹ `confidence_dist.png`: æ¨¡å‹æ˜¯å¦å¯¹é”™è¯¯é¢„æµ‹è¿‡äºè‡ªä¿¡ï¼Ÿ
   - æŸ¥çœ‹ `confusion_matrix.png`: ç³»ç»Ÿæ€§é”™è¯¯æ¨¡å¼

---

## â“ å¸¸è§é—®é¢˜

### Q1: CUDA out of memory

**ç—‡çŠ¶**: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# æ–¹æ¡ˆ1: å‡å°æ‰¹å¤§å°
data:
  batch_size: 256  # åŸæ¥1024

# æ–¹æ¡ˆ2: å‡å°‘workers
data:
  num_workers: 2   # åŸæ¥4

# æ–¹æ¡ˆ3: ç¦ç”¨æ··åˆç²¾åº¦ï¼ˆå¦‚æœGPUä¸æ”¯æŒï¼‰
amp: false

# æ–¹æ¡ˆ4: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
model:
  name: "ResNet1D"
  dropout: 0.1
```

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢

**åŸå› åˆ†æ**:
1. æ•°æ®åŠ è½½ç“¶é¢ˆ
2. GPUåˆ©ç”¨ç‡ä½
3. é‡‡æ ·ç­–ç•¥å¼€é”€å¤§

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# åŠ é€Ÿæ•°æ®åŠ è½½
data:
  in_memory: true      # å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
  num_workers: 8       # å¢åŠ workeræ•°
  pin_memory: true     # å¯ç”¨pin_memory

# å¯ç”¨æ··åˆç²¾åº¦
amp: true

# å‡å°‘éªŒè¯é¢‘ç‡ï¼ˆä¿®æ”¹ä»£ç ï¼‰
# åœ¨main.pyä¸­ï¼Œæ·»åŠ  if epoch % 5 == 0: æ¥æ§åˆ¶éªŒè¯
```

### Q3: éªŒè¯é›†å‡†ç¡®ç‡ä¸æå‡

**å¯èƒ½åŸå› **:
1. å­¦ä¹ ç‡è¿‡å¤§/è¿‡å°
2. è¿‡æ‹Ÿåˆ
3. æ•°æ®å¢å¼ºä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# è°ƒæ•´å­¦ä¹ ç‡
training:
  lr: 5e-4          # å°è¯•æ›´å°çš„å­¦ä¹ ç‡

# å¢åŠ æ­£åˆ™åŒ–
training:
  weight_decay: 5e-4  # å¢åŠ æƒé‡è¡°å‡
  
model:
  dropout: 0.3      # å¢åŠ dropout

# æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ
early_stopping:
  patience: 15      # å‡å°patience
```

### Q4: Stage-2æ•ˆæœä¸æ˜æ˜¾

**å¯èƒ½åŸå› **:
1. Stage-1è®­ç»ƒä¸å……åˆ†
2. Stage-2ç­–ç•¥é€‰æ‹©ä¸å½“
3. è¶…å‚æ•°ä¸åˆé€‚

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# ç¡®ä¿Stage-1å……åˆ†è®­ç»ƒ
training:
  epochs: 300       # å¢åŠ epoch

# CRTæ¨¡å¼é…ç½®
stage2:
  mode: "crt"
  epochs: 200       # Stage-2ä¹Ÿè¦è¶³å¤Ÿé•¿
  lr: 1e-4          # å­¦ä¹ ç‡è¦æ¯”Stage-1å°
  
  # ç»„åˆç­–ç•¥
  loss: "CostSensitiveCE"
  sampler: "progressive_power"
```

### Q5: å°‘æ•°ç±»æ€§èƒ½ä»ç„¶å¾ˆå·®

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# æ›´æ¿€è¿›çš„é‡‡æ ·
sampling:
  name: "class_uniform"  # ç±»å‡åŒ€é‡‡æ ·

# æ›´é«˜çš„ä»£ä»·æƒé‡
loss:
  name: "CostSensitiveCE"
  cost_strategy: "auto"  # è‡ªåŠ¨é€†é¢‘ç‡

# Stage-2ä¹Ÿä½¿ç”¨å¼ºå¹³è¡¡ç­–ç•¥
stage2:
  sampler: "class_uniform"
  loss: "CostSensitiveFocal"
  focal_gamma: 3.0         # å¢å¤§gamma
```

### Q6: æ¨ç†æ—¶æ€§èƒ½ä¸‹é™

**åŸå› **: è®­ç»ƒå’Œæµ‹è¯•åˆ†å¸ƒä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# å¯ç”¨æ¨ç†æ—¶åéªŒè°ƒæ•´
evaluation:
  eval_logit_adjust: "posthoc"
  eval_logit_tau: 1.0

# æˆ–ä½¿ç”¨Stage-3
stage3:
  mode: "logit_adjust"
  logit_tau: 1.0
```

### Q7: æ— æ³•è¯»å–æ•°æ®æ–‡ä»¶

**é”™è¯¯ä¿¡æ¯**: `FileNotFoundError` æˆ– `KeyError`

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥æ–‡ä»¶è·¯å¾„
import os
print(os.path.exists('data/train.mat'))  # åº”è¯¥è¿”å›True

# æ£€æŸ¥MATæ–‡ä»¶å†…å®¹
import scipy.io
data = scipy.io.loadmat('data/train.mat')
print(data.keys())  # æŸ¥çœ‹å¯ç”¨çš„é”®

# æˆ–ç”¨h5pyï¼ˆv7.3æ ¼å¼ï¼‰
import h5py
with h5py.File('data/train.mat', 'r') as f:
    print(list(f.keys()))
```

ç„¶ååœ¨é…ç½®ä¸­æŒ‡å®šæ­£ç¡®çš„é”®ï¼š
```yaml
# å¦‚æœä½ çš„æ•°æ®é”®åä¸æ˜¯X/Y
# éœ€è¦åœ¨ä»£ç ä¸­ä¿®æ”¹ADSBSignalDatasetçš„data_keyå’Œlabel_keyå‚æ•°
```

### Q8: å¤šGPUè®­ç»ƒæŠ¥é”™

**é”™è¯¯**: `RuntimeError: Expected all tensors to be on the same device`

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# ç¡®ä¿æ­£ç¡®æŒ‡å®šGPU
gpus: "0,1,2,3"  # ä½¿ç”¨çš„GPU ID

# æ£€æŸ¥ä»£ç ä¸­çš„DataParallel
# åœ¨main.pyä¸­å·²æ­£ç¡®å¤„ç†ï¼š
# if torch.cuda.is_available() and len(gpu_ids) > 1:
#     model = nn.DataParallel(model, device_ids=gpu_ids)
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### è®­ç»ƒåŠ é€ŸæŠ€å·§

#### 1. æ··åˆç²¾åº¦è®­ç»ƒ
```yaml
amp: true  # å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦

# é¢„æœŸåŠ é€Ÿï¼š1.5-2xï¼ˆåœ¨æ”¯æŒTensor Coreçš„GPUä¸Šï¼‰
```

#### 2. æ•°æ®åŠ è½½ä¼˜åŒ–
```yaml
data:
  in_memory: true      # å°æ•°æ®é›†å…¨åŠ è½½åˆ°å†…å­˜
  num_workers: 8       # CPUæ ¸å¿ƒæ•° - 2
  pin_memory: true     # CUDAåŠ é€Ÿ
  drop_last: true      # è·³è¿‡æœ€åä¸å®Œæ•´çš„batch
```

#### 3. æ¨¡å‹ä¼˜åŒ–
```python
# ä½¿ç”¨æ›´é«˜æ•ˆçš„å½’ä¸€åŒ–
model:
  norm_kind: "gn"  # GroupNormæ¯”BatchNormåœ¨å°batchä¸‹æ›´ç¨³å®š
```

#### 4. æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batchï¼‰
```python
# åœ¨train_eval.pyä¸­æ·»åŠ ï¼ˆå¦‚æœéœ€è¦ï¼‰
accumulation_steps = 4
for i, (x, y) in enumerate(loader):
    loss = criterion(model(x), y) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### å†…å­˜ä¼˜åŒ–

#### 1. å‡å°‘æ˜¾å­˜å ç”¨
```yaml
# å°batch + æ¢¯åº¦ç´¯ç§¯
data:
  batch_size: 256

# ä¸åŠ è½½å…¨éƒ¨æ•°æ®åˆ°å†…å­˜
data:
  in_memory: false

# t-SNEåªé‡‡æ ·éƒ¨åˆ†æ•°æ®
visualization:
  max_samples: 1000
```

#### 2. æ¨¡å‹æ£€æŸ¥ç‚¹ç­–ç•¥
```python
# åªä¿å­˜æœ€ä½³æ¨¡å‹
checkpointer = ModelCheckpointer(
    save_dir=ckpt_dir,
    save_best=True,
    save_interval=0,      # ä¸ä¿å­˜ä¸­é—´epoch
    max_checkpoints=1     # åªä¿ç•™1ä¸ª
)
```

### å®éªŒæ•ˆç‡æå‡

#### 1. å¿«é€ŸåŸå‹éªŒè¯
```yaml
# å°è§„æ¨¡å¿«é€Ÿå®éªŒ
training:
  epochs: 50           # å‡å°‘epoch
  
visualization:
  enabled: false       # å…ˆä¸ç”Ÿæˆå›¾è¡¨
  
stage2:
  enabled: false       # å…ˆåªæµ‹Stage-1
```

#### 2. å¹¶è¡Œå®éªŒï¼ˆå¤šå¡ï¼‰
```bash
# ä¸åŒGPUè¿è¡Œä¸åŒå®éªŒ
CUDA_VISIBLE_DEVICES=0 python main.py exp_name=exp1 &
CUDA_VISIBLE_DEVICES=1 python main.py exp_name=exp2 &
CUDA_VISIBLE_DEVICES=2 python main.py exp_name=exp3 &
wait
```

#### 3. ä½¿ç”¨tmux/screenæŒä¹…åŒ–è®­ç»ƒ
```bash
# åˆ›å»ºä¼šè¯
tmux new -s training

# è¿è¡Œè®­ç»ƒ
python main.py exp_name=long_run

# åˆ†ç¦»ä¼šè¯ï¼ˆè®­ç»ƒç»§ç»­ï¼‰
Ctrl+B, D

# é‡æ–°è¿æ¥
tmux attach -t training
```

---

## ğŸ“š å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ç›¸å…³è®ºæ–‡ï¼š

### æ ¸å¿ƒæ–¹æ³•è®ºæ–‡

**é‡é‡‡æ ·æ–¹æ³•**:
```bibtex
@inproceedings{cui2019classbalanced,
  title={Class-Balanced Loss Based on Effective Number of Samples},
  author={Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  booktitle={CVPR},
  year={2019}
}
```

**æŸå¤±å‡½æ•°**:
```bibtex
@inproceedings{cao2019ldam,
  title={Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss},
  author={Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{menon2021logit,
  title={Long-tail Learning via Logit Adjustment},
  author={Menon, Aditya Krishna and Jayasumana, Sadeep and Rawat, Ankit Singh and Jain, Himanshu and Veit, Andreas and Kumar, Sanjiv},
  booktitle={ICLR},
  year={2021}
}
```

**ä¸¤é˜¶æ®µè®­ç»ƒ**:
```bibtex
@inproceedings{kang2020decoupling,
  title={Decoupling Representation and Classifier for Long-Tailed Recognition},
  author={Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
  booktitle={ICLR},
  year={2020}
}
```

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ **MIT License** å¼€æºåè®®ã€‚

```
MIT License

Copyright (c) 2025 [Your Name/Organization]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘ŠBugæˆ–æå‡ºæ–°åŠŸèƒ½å»ºè®®ï¼

### è´¡çŒ®æ–¹å¼

1. **Fork** æœ¬ä»“åº“
2. åˆ›å»ºä½ çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤ä½ çš„ä¿®æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ä¸€ä¸ª **Pull Request**

### ä»£ç è§„èŒƒ

- éµå¾ªPEP 8ä»£ç é£æ ¼
- æ·»åŠ å¿…è¦çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
- åŒ…å«å•å…ƒæµ‹è¯•ï¼ˆå¦‚é€‚ç”¨ï¼‰
- æ›´æ–°READMEï¼ˆå¦‚æœ‰æ–°åŠŸèƒ½ï¼‰

---

## ğŸ“® è”ç³»æ–¹å¼

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Email**: your.email@example.com
- **è®ºæ–‡**: [arXivé“¾æ¥]

---

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åœ¨å¼€å‘è¿‡ç¨‹ä¸­å‚è€ƒäº†ä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®ï¼š

- [PyTorch](https://pytorch.org/)
- [scikit-learn](https://scikit-learn.org/)
- [Hydra](https://hydra.cc/)
- å„ç§CVPR/NeurIPSè®ºæ–‡çš„å®˜æ–¹å®ç°

ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…å’Œä½¿ç”¨è€…çš„åé¦ˆï¼

---

## ğŸ“Œ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-01-08)
- âœ¨ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- ğŸ¯ é›†æˆ15+ç§ä¸å¹³è¡¡å­¦ä¹ æ–¹æ³•
- ğŸ“Š Nature/ScienceæœŸåˆŠçº§å¯è§†åŒ–
- ğŸš€ ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶
- ğŸ“ å®Œæ•´æ–‡æ¡£å’Œç¤ºä¾‹

### TODO
- [ ] æ·»åŠ æ›´å¤šæ¨¡å‹æ¶æ„ï¼ˆTransformerã€GNNç­‰ï¼‰
- [ ] æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»
- [ ] é›†æˆè‡ªåŠ¨è¶…å‚æ•°æœç´¢ï¼ˆOptunaï¼‰
- [ ] æ·»åŠ çŸ¥è¯†è’¸é¦æ”¯æŒ
- [ ] åœ¨çº¿å­¦ä¹ /å¢é‡å­¦ä¹ æ¨¡å¼
- [ ] Dockerå®¹å™¨åŒ–éƒ¨ç½²

---

<div align="center">

**â­ å¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒä¸€ä¸‹ï¼â­**

Made with â¤ï¸ by [Your Name]

</div>