# 不平衡学习评估指标完全解析

## 📋 目录

- [整体指标 (Overall Metrics)](#整体指标)
- [分组指标 (Group-wise Metrics)](#分组指标)
- [每类指标 (Per-class Metrics)](#每类指标)
- [混淆矩阵 (Confusion Matrix)](#混淆矩阵)
- [高级指标](#高级指标)
- [指标选择建议](#指标选择建议)
- [实例解析](#实例解析)

---

## 整体指标 (Overall Metrics)

### 1. Overall Accuracy (OA) - 总体准确率

**定义**:
```
OA = 正确预测的样本数 / 总样本数
   = (TP + TN) / (TP + TN + FP + FN)
```

**计算示例**:
```python
# 假设有1000个样本，950个预测正确
OA = 950 / 1000 = 0.95 = 95.0%
```

**含义**:
- 最常见的分类指标
- 表示整体预测正确的比例

**不平衡场景下的问题** ⚠️:
```
场景: 100个样本，90个类别A，10个类别B
模型: 把所有样本都预测为A

结果:
- OA = 90/100 = 90%  ← 看起来很好！
- 但类别B的准确率 = 0%  ← 完全失败！

结论: OA在不平衡数据上有严重误导性！
```

**使用建议**:
- ✅ 作为参考指标，但不能单独使用
- ❌ 不适合作为不平衡学习的主要评价标准
- 🔍 必须结合其他指标（mAcc, HM等）

---

### 2. Mean Per-Class Accuracy (mAcc) - 平均类别准确率

**定义**:
```
mAcc = (1/C) × Σ(每个类别的准确率)
     = (Acc_class1 + Acc_class2 + ... + Acc_classC) / C

每个类别的准确率 = 该类正确预测数 / 该类总样本数
                  = Recall_i (召回率)
```

**计算示例**:
```python
# 3个类别，样本数分别为: 100, 50, 10
# 正确预测数分别为:     90,  40,  8

类别1准确率 = 90/100  = 90%
类别2准确率 = 40/50   = 80%
类别3准确率 = 8/10    = 80%

mAcc = (90 + 80 + 80) / 3 = 83.33%

# 对比OA:
OA = (90+40+8) / (100+50+10) = 138/160 = 86.25%
```

**为什么mAcc更重要？** ⭐⭐⭐⭐⭐

```
场景对比:

模型A (只关注多数类):
  类别1 (100样本): 准确率 95%
  类别2 (50样本):  准确率 80%
  类别3 (10样本):  准确率 30%  ← 少数类很差
  
  OA   = (95+40+3)/160 = 86.25%
  mAcc = (95+80+30)/3  = 68.33%

模型B (平衡优化):
  类别1 (100样本): 准确率 90%
  类别2 (50样本):  准确率 85%
  类别3 (10样本):  准确率 80%  ← 少数类好
  
  OA   = (90+42.5+8)/160 = 87.81%
  mAcc = (90+85+80)/3    = 85.00%  ← 明显更好！

结论: mAcc能反映模型在所有类别上的平衡表现
```

**使用建议**:
- ⭐⭐⭐⭐⭐ **不平衡学习的核心指标**
- ✅ 论文中必报指标
- ✅ 给予每个类别相同的重要性
- 🎯 追求mAcc提升是正确的优化方向

---

### 3. Balanced Accuracy - 平衡准确率

**定义**:
```
Balanced Accuracy = (1/C) × Σ(每个类别的召回率)
                  = (Recall_1 + Recall_2 + ... + Recall_C) / C
                  = mAcc (实际上就是mAcc的另一个名字)
```

**Scikit-learn实现**:
```python
from sklearn.metrics import balanced_accuracy_score

balanced_acc = balanced_accuracy_score(y_true, y_pred)
# 等价于每个类别召回率的平均值
```

**与mAcc的关系**:
- 数学上完全等价
- 名字来源于sklearn库的命名
- 在代码中可能同时出现（为了兼容性）

---

### 4. Macro F1-Score - 宏平均F1分数

**定义**:
```
F1_i = 2 × (Precision_i × Recall_i) / (Precision_i + Recall_i)

Macro F1 = (F1_1 + F1_2 + ... + F1_C) / C
```

**各指标关系**:
```
Precision (精确率) = TP / (TP + FP)  "预测为正的有多少是对的"
Recall (召回率)    = TP / (TP + FN)  "实际为正的有多少被找到"
F1-Score          = 精确率和召回率的调和平均
```

**计算示例**:
```python
类别1: Precision=0.90, Recall=0.88
       F1_1 = 2×(0.90×0.88)/(0.90+0.88) = 0.8899

类别2: Precision=0.85, Recall=0.82
       F1_2 = 2×(0.85×0.82)/(0.85+0.82) = 0.8348

类别3: Precision=0.75, Recall=0.80
       F1_3 = 2×(0.75×0.80)/(0.75+0.80) = 0.7742

Macro F1 = (0.8899 + 0.8348 + 0.7742) / 3 = 0.8330 = 83.30%
```

**F1 vs Recall (mAcc)**:
```
场景: 模型过度预测某个类别

类别A (实际100个):
  预测为A: 200个 (包含100个正确 + 100个误判)
  Recall    = 100/100 = 100%  ← 很好！
  Precision = 100/200 = 50%   ← 但精确率低
  F1        = 66.7%           ← 综合考虑

结论: F1综合考虑了"找全"和"找准"
```

**使用建议**:
- ⭐⭐⭐⭐ 综合性指标，推荐使用
- ✅ 同时考虑精确率和召回率
- ✅ 对类别不平衡有一定鲁棒性
- 📊 适合实际应用中的性能评估

---

### 5. Micro F1-Score - 微平均F1分数

**定义**:
```
Micro Precision = Σ(TP_i) / Σ(TP_i + FP_i)
Micro Recall    = Σ(TP_i) / Σ(TP_i + FN_i)
Micro F1        = 2 × (Micro_P × Micro_R) / (Micro_P + Micro_R)
```

**与Macro F1的区别**:
```
Macro F1: 先算每类F1，再平均  → 每个类别权重相同
Micro F1: 先汇总所有TP/FP/FN，再算F1 → 每个样本权重相同

在多分类中: Micro F1 = Overall Accuracy
```

**使用建议**:
- 在不平衡学习中很少单独使用
- 主要用于多标签分类场景

---

### 6. G-Mean (Geometric Mean of Recalls) - 召回率几何平均

**定义**:
```
G-Mean = (Recall_1 × Recall_2 × ... × Recall_C)^(1/C)
       = C次方根(所有类别召回率的乘积)
```

**计算示例**:
```python
类别1 Recall = 90% = 0.90
类别2 Recall = 85% = 0.85
类别3 Recall = 20% = 0.20  ← 很差

G-Mean = (0.90 × 0.85 × 0.20)^(1/3)
       = (0.153)^(1/3)
       = 0.535 = 53.5%

# 对比算术平均(mAcc):
mAcc = (0.90 + 0.85 + 0.20) / 3 = 0.65 = 65%
```

**特点**:
```
几何平均的特性: 如果任何一个类别性能为0，G-Mean就是0

示例:
  类别1,2,3的Recall: 100%, 100%, 0%
  mAcc   = (1 + 1 + 0) / 3 = 66.7%  ← 还不错？
  G-Mean = (1 × 1 × 0)^(1/3) = 0%   ← 明确失败！

优点: 对极差的类别更敏感，惩罚不平衡
缺点: 可能过于严格
```

**使用建议**:
- ⭐⭐⭐ 理论上重要，文献常见
- ⚠️ 对单个类别失败极其敏感
- 🔍 适合检测是否有"灾难性失败"的类别

---

### 7. Top-K Accuracy - 前K准确率

**定义**:
```
Top-K Acc = 真实类别在模型Top-K预测中的样本数 / 总样本数
```

**示例**:
```python
样本1真实类别: 类别3
模型预测概率: [0.2, 0.35, 0.25, 0.15, 0.05]
           类别: [1,   2,    3,    4,    5]
           排序: [2,   3,    1,    4,    5]

Top-1预测: 类别2 → 错误
Top-3预测: 类别2,3,1 → 包含类别3 → 正确！

Top-1 Accuracy: 传统准确率
Top-5 Accuracy: 真实类别在前5个预测中即算对
```

**应用场景**:
- 类别数很多时（如ImageNet 1000类）
- 提供"容错"的评估
- 反映模型的"候选能力"

---

### 8. AUROC & AUPRC

#### AUROC (Area Under ROC Curve) - ROC曲线下面积

**定义**:
```
ROC曲线: TPR vs FPR 在不同阈值下的曲线
TPR (True Positive Rate)  = Recall = TP/(TP+FN)
FPR (False Positive Rate) = FP/(FP+TN)

AUROC = ROC曲线与x轴围成的面积
```

**解释**:
- AUROC = 0.5: 随机猜测
- AUROC = 1.0: 完美分类器
- AUROC = 0.7-0.8: 可接受
- AUROC = 0.8-0.9: 良好
- AUROC > 0.9: 优秀

**多分类AUROC**:
```python
# One-vs-Rest策略
from sklearn.metrics import roc_auc_score

# 为每个类别计算AUROC，然后平均
macro_auroc = roc_auc_score(y_true, y_prob, 
                            multi_class='ovr', 
                            average='macro')
```

#### AUPRC (Area Under PR Curve) - PR曲线下面积

**定义**:
```
PR曲线: Precision vs Recall 在不同阈值下的曲线

AUPRC = PR曲线与x轴围成的面积
     = Average Precision (AP)
```

**AUROC vs AUPRC在不平衡数据上**:
```
场景: 1000个样本，990个负例，10个正例

假设模型预测:
  - 8个正例正确识别（TP=8, FN=2）
  - 50个负例被误判为正（FP=50, TN=940）

AUROC: 考虑TN，可能仍然很高(>0.9)
       因为TN=940很大，分母大

AUPRC: 不考虑TN，会更低
       Precision = 8/(8+50) = 13.8%
       受误判影响大

结论: 在极端不平衡时，AUPRC比AUROC更严格
```

**使用建议**:
- AUROC: 平衡数据或中等不平衡
- AUPRC: 极端不平衡（正例<5%）

---

## 分组指标 (Group-wise Metrics)

### 分组策略

代码中自动将类别分为3组：

```python
# 分组逻辑
if max(class_counts) >= 100:
    # 绝对阈值
    多数类 (majority):  样本数 >= 100
    少数类 (minority):  样本数 <= 20
    中等类 (medium):    介于两者之间
else:
    # 相对阈值（分位数）
    多数类: 样本数 >= 2/3分位数
    少数类: 样本数 <= 1/3分位数
    中等类: 介于两者之间
```

**示例**:
```
10个类别的样本数: [500, 400, 300, 200, 150, 100, 50, 30, 20, 10]

分组结果:
  多数类: [500, 400, 300, 200, 150, 100]  ← 前6个类
  中等类: [50, 30]                       ← 中间2个类
  少数类: [20, 10]                       ← 后2个类
```

### 分组指标内容

每个组报告以下指标：

```json
{
  "majority": {
    "accuracy": 92.5,           // 该组所有类别样本的整体准确率
    "balanced_accuracy": 91.8,  // 该组类别的平衡准确率
    "precision": 90.2,          // 该组类别精确率的平均
    "recall": 91.8,             // 该组类别召回率的平均
    "f1": 90.9,                 // 该组类别F1的平均
    "support": 1650,            // 该组总样本数
    "worst_class_recall": 88.0, // 该组中最差的类别召回率
    "top5": 95.3                // Top-5准确率（如果适用）
  }
}
```

### 为什么需要分组指标？

```
目的: 观察模型在不同数据规模类别上的表现差异

理想情况:
  多数类准确率: 95%
  中等类准确率: 93%
  少数类准确率: 90%  ← 差距不大，平衡良好

常见问题:
  多数类准确率: 98%
  中等类准确率: 85%
  少数类准确率: 45%  ← 严重不平衡！

分析价值:
  - 发现性能瓶颈在哪类样本
  - 评估不平衡学习方法是否有效
  - 指导进一步改进方向
```

---

## 每类指标 (Per-class Metrics)

### 内容

```json
{
  "0": {  // 类别0
    "precision": 92.5,    // 该类别的精确率
    "recall": 90.3,       // 该类别的召回率
    "f1": 91.4,           // 该类别的F1分数
    "support": 45,        // 测试集中该类别的样本数
    "frequency": 500      // 训练集中该类别的样本数
  },
  "1": {  // 类别1
    ...
  }
}
```

### 关键诊断

#### 1. 找出最差的类别
```python
worst_class = min(per_class.items(), 
                  key=lambda x: x[1]['recall'])
print(f"最差类别: {worst_class[0]}, 召回率: {worst_class[1]['recall']}")
```

#### 2. 分析性能与样本数的关系
```python
import matplotlib.pyplot as plt

frequencies = [per_class[str(i)]['frequency'] for i in range(num_classes)]
recalls = [per_class[str(i)]['recall'] for i in range(num_classes)]

plt.scatter(frequencies, recalls)
plt.xlabel('Training Samples')
plt.ylabel('Recall (%)')
plt.xscale('log')  # 对数坐标看趋势
```

预期模式：
```
好的模型: 性能与样本数有正相关，但不至于极端
差的模型: 少数类性能断崖式下跌
```

#### 3. Precision vs Recall 分析

```
高Precision低Recall:
  → 模型保守，只在非常确定时才预测该类
  → 可能需要降低阈值或增加该类的权重

低Precision高Recall:
  → 模型激进，大量预测该类但很多是错的
  → 可能该类与其他类混淆严重，需要更好的特征

Precision和Recall都低:
  → 模型在该类上整体失败
  → 需要重点关注（数据、特征、模型）
```

---

## 混淆矩阵 (Confusion Matrix)

### 结构

```
            预测
        0   1   2   3
真  0  [90   5   3   2]   ← 类别0: 90个对，5个误判为1...
实  1  [ 8  80   4   8]
    2  [ 2   6  85   7]
    3  [ 5  10   5  80]
```

### 归一化混淆矩阵

```
按行归一化（按真实类别归一化）:

            预测
        0     1     2     3
真  0  [0.90  0.05  0.03  0.02]  ← 90%正确
实  1  [0.08  0.80  0.04  0.08]  ← 80%正确
    2  [0.02  0.06  0.85  0.07]
    3  [0.05  0.10  0.05  0.80]

对角线 = 每个类别的召回率
```

### 从混淆矩阵诊断问题

#### 1. 系统性混淆
```
现象: 类别A经常被误判为类别B，但反过来不明显

        预测
        A    B
真  A  [70   30]  ← A→B误判率高
实  B  [ 5   95]  ← B→A误判率低

可能原因:
  - 类别A的特征不够区分性
  - 类别B的样本数远多于A（模型偏向多数类）
  - 类别A可能是类别B的子集

解决方案:
  - 增加类别A的训练样本
  - 使用代价敏感损失（提高A→B误判的代价）
  - 改进特征提取
```

#### 2. 多类混淆
```
现象: 某个类别同时与多个类别混淆

        预测
        0    1    2    3
真  2  [15   20   40   25]  ← 类别2的性能很差

可能原因:
  - 类别2本身定义模糊
  - 类别2的样本质量差（噪声多）
  - 模型特征空间中类别2没有明确的聚类

解决方案:
  - 检查数据标注质量
  - 单独分析类别2的样本特征
  - 考虑层次化分类
```

#### 3. 对角线外的亮点
```
        0    1    2
    0  [90   5    5]
    1  [ 3  92    5]
    2  [ 2   3   95]  ← 对角线清晰，混淆少
    
好的混淆矩阵: 对角线亮，其余暗
```

---

## 高级指标

### 1. Harmonic Mean (HM) - 多数类vs少数类调和平均

**定义**:
```
HM = 2 × (Many_Acc × Few_Acc) / (Many_Acc + Few_Acc)
```

**为什么用调和平均而不是算术平均？**

```
场景1: 牺牲少数类来提升多数类
  Many_Acc = 98%
  Few_Acc  = 30%
  算术平均 = (98+30)/2 = 64%     ← 看起来还行
  调和平均 = 2×98×30/(98+30) = 45.9%  ← 反映了严重不平衡

场景2: 平衡优化
  Many_Acc = 90%
  Few_Acc  = 85%
  算术平均 = 87.5%
  调和平均 = 87.4%               ← 接近算术平均

特点: 调和平均对较小的值更敏感，惩罚极端不平衡
```

**使用建议**:
- ⭐⭐⭐⭐⭐ **不平衡学习最重要的综合指标之一**
- ✅ 反映头部和尾部的平衡
- 🎯 优化目标应该包括HM

### 2. Worst Class Recall - 最差类别召回率

**定义**:
```
Worst_Class_Recall = min(Recall_1, Recall_2, ..., Recall_C)
```

**意义**:
```
检测"木桶短板"效应

示例:
  类别1-9召回率: 95%, 93%, 92%, ..., 90%
  类别10召回率: 20%  ← 灾难性失败

  mAcc = 87.5%  ← 平均值掩盖了问题
  Worst = 20%   ← 明确指出最弱环节
```

**使用建议**:
- 🔍 诊断性指标
- ⚠️ 如果worst recall < 50%，说明有严重问题
- 🎯 改进方向：专注提升最差类别

### 3. 样本数 vs 性能相关系数

**在结果中体现为拟合曲线的R²**

```python
# 代码中计算（visualization.py）
log_counts = np.log10(class_counts)
recalls = [per_class_recall for each class]

# 线性拟合
z = np.polyfit(log_counts, recalls, 1)
R² = correlation_coefficient ** 2
```

**解释**:
```
R² = 0.9: 性能强烈依赖样本数（不平衡学习失败）
R² = 0.3: 性能与样本数相关性弱（不平衡学习成功）

目标: 降低R²，使性能更少依赖样本数量
```

---

## 指标选择建议

### 不同场景下的主要指标

#### 场景1: 学术研究/论文

**必须报告**:
1. ⭐⭐⭐⭐⭐ mAcc (Mean Per-Class Accuracy)
2. ⭐⭐⭐⭐⭐ Macro F1
3. ⭐⭐⭐⭐⭐ Many-Acc, Few-Acc, HM
4. ⭐⭐⭐⭐ Balanced Accuracy

**建议报告**:
5. ⭐⭐⭐ G-Mean
6. ⭐⭐⭐ Overall Accuracy (作为参考)
7. ⭐⭐ Top-5 Accuracy (如果类别数>50)

**可选报告**:
- AUROC/AUPRC (如果类别数较少)
- 混淆矩阵可视化
- 每类性能条形图

#### 场景2: 实际应用部署

**优先级1 - 业务关键**:
1. Few-Acc (少数类准确率) - 往往是业务关注的重点
2. Macro F1 - 综合性能
3. Worst Class Recall - 最差类别表现

**优先级2 - 系统稳定性**:
4. 混淆矩阵 - 了解误判模式
5. 置信度分布 - 模型可靠性
6. Top-K准确率 - 人机协作场景

#### 场景3: 模型调优

**快速评估**:
1. mAcc - 主要优化目标
2. HM - 平衡性检查
3. Worst Class Recall - 找问题

**深入分析**:
4. Per-class metrics - 定位具体问题类别
5. Group-wise metrics - 分析不同规模类别的表现
6. 样本数vs性能图 - 评估数据依赖

#### 场景4: 不同不平衡程度

```
轻度不平衡 (IR < 10):
  主要: OA, mAcc, Macro F1
  
中度不平衡 (10 < IR < 100):
  主要: mAcc, HM, Few-Acc
  次要: Balanced Acc, G-Mean
  
重度不平衡 (IR > 100):
  主要: Few-Acc, Worst Class Recall
  次要: mAcc, HM, AUPRC
```

---

## 实例解析

### 完整示例解读

假设实验输出：

```
==================================================================
实验结果概览
==================================================================

【整体指标】
  Overall Accuracy (OA)        :  86.50%
  Mean Per-Class Accuracy (mAcc):  78.30%
  Macro-F1                     :  76.80%
  Balanced Accuracy            :  78.30%

【分组指标】
  Majority   Acc:  93.20%  F1:  92.50%  Support: 3500
  Medium     Acc:  81.40%  F1:  79.60%  Support:  800
  Minority   Acc:  65.30%  F1:  62.10%  Support:  200

【Head vs Tail】
  Head (Many)      :  93.20%
  Tail (Few)       :  65.30%
  Harmonic Mean    :  76.80%
```

### 逐项分析

#### 1. OA vs mAcc 差距分析
```
OA   = 86.50%
mAcc = 78.30%
差距 = 8.20%

解读:
- 差距较大(>5%)说明存在明显不平衡
- OA被多数类"拉高"了
- mAcc更真实反映了性能
```

#### 2. 分组性能梯度
```
Majority (93.20%) → Medium (81.40%) → Minority (65.30%)
       差12%                差16%

解读:
- 性能随样本数递减，这是正常的
- 但少数类下降幅度更大(16% vs 12%)
- 说明模型在极少样本类别上有困难
```

#### 3. HM分析
```
Many-Acc = 93.20%
Few-Acc  = 65.30%
HM       = 76.80%

计算: 2 × 93.20 × 65.30 / (93.20 + 65.30) = 76.80% ✓

解读:
- HM更接近Few-Acc (受较小值主导)
- 如果Few-Acc提升到75%，HM会提升到83%
- 优化方向明确：提升少数类性能
```

#### 4. 改进空间评估

```
当前状态:
  mAcc = 78.30%
  HM   = 76.80%

目标设定:
  情景A (保守): Few-Acc从65%→75% → HM提升到83%
  情景B (激进): Few-Acc从65%→85% → HM提升到89%

实施策略:
  1. Stage-2采用Cost-Sensitive损失 (预期+5-10%)
  2. Progressive Sampling (预期+3-5%)
  3. 组合使用 (预期+10-15%)
```

### 对比两个模型

```
模型A (Baseline):
  OA: 87.0%  mAcc: 75.0%  HM: 73.0%  Few-Acc: 60.0%

模型B (改进后):
  OA: 85.5%  mAcc: 82.0%  HM: 81.0%  Few-Acc: 78.0%

分析:
  ❌ OA下降了1.5% - 多数类性能略有牺牲
  ✅ mAcc提升了7%  - 整体平衡性大幅提升
  ✅ HM提升了8%    - 头尾平衡显著改善
  ✅ Few-Acc提升18% - 少数类性能质的飞跃

结论: 模型B更好！虽然OA略降，但在不平衡学习中
      我们更关心平衡性能(mAcc, HM, Few-Acc)
```

---

## 总结：一张表看懂所有指标

| 指标 | 计算方式 | 不平衡敏感度 | 主要用途 | 重要性 |
|------|---------|------------|---------|--------|
| **OA** | 正确数/总数 | ❌ 低 | 参考对比 | ⭐⭐ |
| **mAcc** | 每类Acc平均 | ✅ 高 | 核心指标 | ⭐⭐⭐⭐⭐ |
| **Macro F1** | 每类F1平均 | ✅ 高 | 综合评估 | ⭐⭐⭐⭐ |
| **Balanced Acc** | =mAcc | ✅ 高 | sklearn标准 | ⭐⭐⭐⭐ |
| **G-Mean** | Recall几何平均 | ✅✅ 极高 | 理论分析 | ⭐⭐⭐ |
| **Few-Acc** | 少数类准确率 | ✅✅ 极高 | 尾部性能 | ⭐⭐⭐⭐⭐ |
| **HM** | Many vs Few调和 | ✅✅ 极高 | 平衡性 | ⭐⭐⭐⭐⭐ |
| **Worst Recall** | 最小的类Recall | ✅✅ 极高 | 找短板 | ⭐⭐⭐⭐ |
| **AUROC** | ROC曲线面积 | ⚠️ 中 | 概率评估 | ⭐⭐⭐ |
| **AUPRC** | PR曲线面积 | ✅ 高 | 极端不平衡 | ⭐⭐⭐ |

### 快速决策树

```
开始
  ↓
OA和mAcc差距大吗？
  ├─ 是(>5%) → 存在严重不平衡，关注mAcc/HM
  └─ 否(<5%) → 轻度不平衡，OA也可参考
  
mAcc满意吗？
  ├─ 是(>80%) → 检查Few-Acc和HM
  └─ 否(<80%) → 需要改进
  
Few-Acc满意吗？
  ├─ 是(>70%) → 模型整体良好
  └─ 否(<70%) → 需要重点优化少数类
      ↓
      查看混淆矩阵和per-class metrics
      ↓
      找出最差的类别
      ↓
      分析原因：数据？模型？还是本质难区分？
      ↓
      针对性改进
```

---

## 附录：代码提取指标

```python
# 从results.json提取关键指标
import json

with open('results.json', 'r') as f:
    results = json.load(f)

# 整体指标
overall = results['test_results']['overall']
print(f"OA: {overall['accuracy']:.2f}%")
print(f"mAcc: {overall['macro_recall']:.2f}%")
print(f"Macro F1: {overall['macro_f1']:.2f}%")

# 分组指标
groups = results['test_results']['group_wise']
for group_name in ['majority', 'minority']:
    if group_name in groups:
        g = groups[group_name]
        print(f"{group_name.capitalize()} Acc: {g['accuracy']:.2f}%")

# 每类指标
per_class = results['test_results']['per_class']
for class_id, metrics in per_class.items():
    print(f"Class {class_id}: Recall={metrics['recall']:.2f}%, "
          f"Samples={metrics['frequency']}")
```

---

**记住**: 在不平衡学习中，**mAcc、HM、Few-Acc** 是最重要的三个指标！